{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 16)                2064      \n",
      "_________________________________________________________________\n",
      "reshape_2 (Reshape)          (None, 4, 4, 1)           0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 4, 4, 1)           4         \n",
      "_________________________________________________________________\n",
      "re_lu_5 (ReLU)               (None, 4, 4, 1)           0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_5 (Conv2DTr (None, 8, 8, 64)          1664      \n",
      "_________________________________________________________________\n",
      "batch_normalization_9 (Batch (None, 8, 8, 64)          256       \n",
      "_________________________________________________________________\n",
      "re_lu_6 (ReLU)               (None, 8, 8, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_6 (Conv2DTr (None, 16, 16, 32)        51232     \n",
      "_________________________________________________________________\n",
      "batch_normalization_10 (Batc (None, 16, 16, 32)        128       \n",
      "_________________________________________________________________\n",
      "re_lu_7 (ReLU)               (None, 16, 16, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_7 (Conv2DTr (None, 32, 32, 16)        12816     \n",
      "_________________________________________________________________\n",
      "batch_normalization_11 (Batc (None, 32, 32, 16)        64        \n",
      "_________________________________________________________________\n",
      "re_lu_8 (ReLU)               (None, 32, 32, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_8 (Conv2DTr (None, 64, 64, 1)         401       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 64, 64, 1)         0         \n",
      "=================================================================\n",
      "Total params: 68,629\n",
      "Trainable params: 68,403\n",
      "Non-trainable params: 226\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 31, 31, 16)        160       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 31, 31, 16)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 15, 15, 32)        4640      \n",
      "_________________________________________________________________\n",
      "batch_normalization_12 (Batc (None, 15, 15, 32)        128       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 7, 7, 64)          18496     \n",
      "_________________________________________________________________\n",
      "batch_normalization_13 (Batc (None, 7, 7, 64)          256       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 7, 7, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 3, 3, 128)         73856     \n",
      "_________________________________________________________________\n",
      "batch_normalization_14 (Batc (None, 3, 3, 128)         512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 3, 3, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 1152)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 1153      \n",
      "=================================================================\n",
      "Total params: 99,201\n",
      "Trainable params: 98,753\n",
      "Non-trainable params: 448\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kanghyun/anaconda3/envs/tensorflow14/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [D loss: 7.320593] [G loss: -0.878033]\n",
      "-0.011090688\n",
      "0.45645028\n",
      "-0.011504922\n",
      "0.46449465\n",
      "-0.017459104\n",
      "0.5053389\n",
      "-0.0300442\n",
      "0.54781616\n",
      "-0.031605825\n",
      "0.5897891\n",
      "-0.009546616\n",
      "0.4879308\n",
      "-0.019634806\n",
      "0.5229346\n",
      "0.030721169\n",
      "0.21787673\n",
      "-0.023107104\n",
      "0.5306151\n",
      "-0.04723733\n",
      "0.6573054\n",
      "0.0021754177\n",
      "0.40651467\n",
      "0.0017096475\n",
      "0.41965955\n",
      "-0.046793263\n",
      "0.6419024\n",
      "-0.0357565\n",
      "0.57741445\n",
      "-0.014038382\n",
      "0.48088464\n",
      "-0.0074513927\n",
      "0.46998549\n",
      "-0.019210164\n",
      "0.5150559\n",
      "-0.037323717\n",
      "0.6119364\n",
      "-0.013174439\n",
      "0.50064886\n",
      "-0.0391314\n",
      "0.6238116\n",
      "0.0024812468\n",
      "0.3756786\n",
      "-0.011193244\n",
      "0.44981408\n",
      "-0.005947946\n",
      "0.42519453\n",
      "-0.0072468887\n",
      "0.45025453\n",
      "-0.021136483\n",
      "0.53453743\n",
      "1 [D loss: 5.902211] [G loss: -0.643576]\n",
      "2 [D loss: 2.604287] [G loss: -0.616813]\n",
      "3 [D loss: 2.629879] [G loss: -0.623206]\n",
      "4 [D loss: 1.780248] [G loss: -0.469307]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kanghyun/anaconda3/envs/tensorflow14/lib/python3.6/site-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 [D loss: 1.642174] [G loss: -0.579649]\n",
      "6 [D loss: 2.066189] [G loss: -0.628549]\n",
      "7 [D loss: 1.665476] [G loss: -0.575991]\n",
      "8 [D loss: 1.223852] [G loss: -0.659522]\n",
      "9 [D loss: 1.239815] [G loss: -0.824047]\n",
      "10 [D loss: 1.310674] [G loss: -0.606367]\n",
      "11 [D loss: 0.712804] [G loss: -0.753943]\n",
      "12 [D loss: 0.531258] [G loss: -0.688750]\n",
      "13 [D loss: 0.508943] [G loss: -0.781948]\n",
      "14 [D loss: 0.781430] [G loss: -0.793914]\n",
      "15 [D loss: 1.176660] [G loss: -0.516755]\n",
      "16 [D loss: 0.806989] [G loss: -0.753576]\n",
      "17 [D loss: 0.663703] [G loss: -0.654667]\n",
      "18 [D loss: 0.898287] [G loss: -0.750795]\n",
      "19 [D loss: 0.873527] [G loss: -0.851103]\n",
      "20 [D loss: 0.971710] [G loss: -0.656588]\n",
      "21 [D loss: 0.629272] [G loss: -0.757773]\n",
      "22 [D loss: 0.648560] [G loss: -0.866699]\n",
      "23 [D loss: 0.990847] [G loss: -0.520735]\n",
      "24 [D loss: 0.589244] [G loss: -0.711491]\n",
      "25 [D loss: 1.291382] [G loss: -0.655621]\n",
      "26 [D loss: 1.043832] [G loss: -0.584641]\n",
      "27 [D loss: 1.154918] [G loss: -0.827213]\n",
      "28 [D loss: 1.233057] [G loss: -0.713756]\n",
      "29 [D loss: 0.553066] [G loss: -0.727565]\n",
      "30 [D loss: 0.495985] [G loss: -0.653034]\n",
      "31 [D loss: 0.557543] [G loss: -0.671323]\n",
      "32 [D loss: 0.895558] [G loss: -0.444806]\n",
      "33 [D loss: 1.282920] [G loss: -0.814273]\n",
      "34 [D loss: 1.132561] [G loss: -0.494797]\n",
      "35 [D loss: 0.919720] [G loss: -0.593933]\n",
      "36 [D loss: 0.841616] [G loss: -0.707808]\n",
      "37 [D loss: 0.661877] [G loss: -0.568962]\n",
      "38 [D loss: 0.705672] [G loss: -0.686658]\n",
      "39 [D loss: 0.669863] [G loss: -0.547231]\n",
      "40 [D loss: 0.468497] [G loss: -0.640202]\n",
      "41 [D loss: 0.511965] [G loss: -0.681539]\n",
      "42 [D loss: 0.532268] [G loss: -0.707818]\n",
      "43 [D loss: 0.614158] [G loss: -0.702518]\n",
      "44 [D loss: 0.648805] [G loss: -0.470248]\n",
      "45 [D loss: 0.615757] [G loss: -0.530588]\n",
      "46 [D loss: 0.659351] [G loss: -0.615866]\n",
      "47 [D loss: 0.479183] [G loss: -0.573586]\n",
      "48 [D loss: 0.346829] [G loss: -0.419016]\n",
      "49 [D loss: 0.390095] [G loss: -0.551752]\n",
      "50 [D loss: 0.458976] [G loss: -0.462757]\n",
      "51 [D loss: 0.443807] [G loss: -0.515983]\n",
      "52 [D loss: 0.405870] [G loss: -0.400174]\n",
      "53 [D loss: 0.542013] [G loss: -0.283546]\n",
      "54 [D loss: 0.452674] [G loss: -0.509192]\n",
      "55 [D loss: 0.881113] [G loss: -0.296242]\n",
      "56 [D loss: 0.356153] [G loss: -0.469971]\n",
      "57 [D loss: 0.470954] [G loss: -0.391974]\n",
      "58 [D loss: 1.255217] [G loss: -0.347843]\n",
      "59 [D loss: 2.604988] [G loss: -0.458331]\n",
      "60 [D loss: 1.012139] [G loss: -0.490525]\n",
      "61 [D loss: 0.485568] [G loss: -0.624777]\n",
      "62 [D loss: 0.492006] [G loss: -0.619603]\n",
      "63 [D loss: 0.571097] [G loss: -0.613014]\n",
      "64 [D loss: 0.622686] [G loss: -0.611552]\n",
      "65 [D loss: 0.999179] [G loss: -0.370490]\n",
      "66 [D loss: 0.432039] [G loss: -0.508828]\n",
      "67 [D loss: 0.395100] [G loss: -0.574902]\n",
      "68 [D loss: 0.531481] [G loss: -0.547785]\n",
      "69 [D loss: 0.358440] [G loss: -0.508379]\n",
      "70 [D loss: 0.407511] [G loss: -0.423745]\n",
      "71 [D loss: 0.371557] [G loss: -0.598989]\n",
      "72 [D loss: 0.295860] [G loss: -0.368818]\n",
      "73 [D loss: 0.391016] [G loss: -0.343980]\n",
      "74 [D loss: 0.217166] [G loss: -0.320967]\n",
      "75 [D loss: 0.321870] [G loss: -0.416280]\n",
      "76 [D loss: 0.495142] [G loss: -0.478444]\n",
      "77 [D loss: 0.955247] [G loss: -0.530689]\n",
      "78 [D loss: 1.085428] [G loss: -0.224915]\n",
      "79 [D loss: 0.498960] [G loss: -0.457148]\n",
      "80 [D loss: 0.259224] [G loss: -0.405941]\n",
      "81 [D loss: 0.189040] [G loss: -0.410874]\n",
      "82 [D loss: 0.336362] [G loss: -0.416831]\n",
      "83 [D loss: 0.466673] [G loss: -0.370122]\n",
      "84 [D loss: 0.255436] [G loss: -0.457229]\n",
      "85 [D loss: 0.481805] [G loss: -0.265188]\n",
      "86 [D loss: 0.787810] [G loss: -0.503066]\n",
      "87 [D loss: 1.531482] [G loss: -0.479521]\n",
      "88 [D loss: 0.736295] [G loss: -0.471104]\n",
      "89 [D loss: 0.654906] [G loss: -0.429239]\n",
      "90 [D loss: 0.596958] [G loss: -0.657612]\n",
      "91 [D loss: 0.709404] [G loss: -0.467982]\n",
      "92 [D loss: 0.382269] [G loss: -0.589790]\n",
      "93 [D loss: 0.549148] [G loss: -0.699123]\n",
      "94 [D loss: 0.568231] [G loss: -0.635702]\n",
      "95 [D loss: 0.826949] [G loss: -0.440731]\n",
      "96 [D loss: 0.642268] [G loss: -0.481188]\n",
      "97 [D loss: 0.543897] [G loss: -0.455378]\n",
      "98 [D loss: 0.347448] [G loss: -0.444926]\n",
      "99 [D loss: 0.332121] [G loss: -0.421699]\n",
      "100 [D loss: 0.538913] [G loss: -0.589497]\n",
      "101 [D loss: 0.391841] [G loss: -0.383488]\n",
      "102 [D loss: 0.797445] [G loss: -0.566113]\n",
      "103 [D loss: 0.877957] [G loss: -0.643618]\n",
      "104 [D loss: 0.811399] [G loss: -0.526655]\n",
      "105 [D loss: 0.599642] [G loss: -0.615266]\n",
      "106 [D loss: 0.477669] [G loss: -0.559224]\n",
      "107 [D loss: 0.486030] [G loss: -0.609803]\n",
      "108 [D loss: 0.584203] [G loss: -0.608295]\n",
      "109 [D loss: 0.734957] [G loss: -0.594960]\n",
      "110 [D loss: 0.827581] [G loss: -0.581230]\n",
      "111 [D loss: 2.433313] [G loss: -0.756509]\n",
      "112 [D loss: 1.206398] [G loss: -0.470143]\n",
      "113 [D loss: 0.550340] [G loss: -0.525725]\n",
      "114 [D loss: 0.744189] [G loss: -0.442643]\n",
      "115 [D loss: 1.106667] [G loss: -0.628928]\n",
      "116 [D loss: 0.585899] [G loss: -0.604510]\n",
      "117 [D loss: 0.528095] [G loss: -0.594425]\n",
      "118 [D loss: 0.634818] [G loss: -0.623910]\n",
      "119 [D loss: 0.575108] [G loss: -0.648251]\n",
      "120 [D loss: 0.810111] [G loss: -0.428882]\n",
      "121 [D loss: 0.335829] [G loss: -0.533184]\n",
      "122 [D loss: 0.453392] [G loss: -0.536867]\n",
      "123 [D loss: 0.368123] [G loss: -0.551943]\n",
      "124 [D loss: 0.498926] [G loss: -0.532841]\n",
      "125 [D loss: 0.438085] [G loss: -0.601655]\n",
      "126 [D loss: 0.547134] [G loss: -0.433136]\n",
      "127 [D loss: 0.567061] [G loss: -0.584733]\n",
      "128 [D loss: 0.427282] [G loss: -0.566332]\n",
      "129 [D loss: 0.499346] [G loss: -0.442181]\n",
      "130 [D loss: 0.611601] [G loss: -0.656491]\n",
      "131 [D loss: 0.672284] [G loss: -0.394608]\n",
      "132 [D loss: 0.599275] [G loss: -0.598835]\n",
      "133 [D loss: 0.639880] [G loss: -0.465744]\n",
      "134 [D loss: 0.741111] [G loss: -0.489718]\n",
      "135 [D loss: 0.513975] [G loss: -0.642526]\n",
      "136 [D loss: 0.991595] [G loss: -0.474459]\n",
      "137 [D loss: 0.756433] [G loss: -0.438668]\n",
      "138 [D loss: 0.436219] [G loss: -0.573566]\n",
      "139 [D loss: 0.390530] [G loss: -0.619670]\n",
      "140 [D loss: 0.485184] [G loss: -0.579800]\n",
      "141 [D loss: 0.390978] [G loss: -0.470033]\n",
      "142 [D loss: 0.378265] [G loss: -0.501430]\n",
      "143 [D loss: 0.519257] [G loss: -0.293367]\n",
      "144 [D loss: 0.370823] [G loss: -0.531376]\n",
      "145 [D loss: 0.409517] [G loss: -0.573384]\n",
      "146 [D loss: 0.696065] [G loss: -0.533450]\n",
      "147 [D loss: 1.089933] [G loss: -0.350789]\n",
      "148 [D loss: 2.222372] [G loss: -0.679162]\n",
      "149 [D loss: 1.346819] [G loss: -0.732219]\n",
      "150 [D loss: 1.434789] [G loss: -0.806792]\n",
      "151 [D loss: 0.769178] [G loss: -0.691414]\n",
      "152 [D loss: 0.608482] [G loss: -0.715700]\n",
      "153 [D loss: 1.113225] [G loss: -0.729033]\n",
      "154 [D loss: 0.544870] [G loss: -0.533623]\n",
      "155 [D loss: 0.752810] [G loss: -0.514001]\n",
      "156 [D loss: 1.181103] [G loss: -0.782678]\n",
      "157 [D loss: 0.612008] [G loss: -0.681028]\n",
      "158 [D loss: 0.624232] [G loss: -0.534108]\n",
      "159 [D loss: 0.430596] [G loss: -0.572782]\n",
      "160 [D loss: 0.413431] [G loss: -0.649402]\n",
      "161 [D loss: 0.473678] [G loss: -0.585316]\n",
      "162 [D loss: 0.605461] [G loss: -0.591396]\n",
      "163 [D loss: 0.662680] [G loss: -0.423576]\n",
      "164 [D loss: 0.935289] [G loss: -0.641288]\n",
      "165 [D loss: 0.713730] [G loss: -0.374209]\n",
      "166 [D loss: 0.485770] [G loss: -0.539470]\n",
      "167 [D loss: 0.751009] [G loss: -0.280662]\n",
      "168 [D loss: 0.308362] [G loss: -0.489524]\n",
      "169 [D loss: 0.354614] [G loss: -0.402582]\n",
      "170 [D loss: 0.585305] [G loss: -0.460301]\n",
      "171 [D loss: 0.539227] [G loss: -0.501314]\n",
      "172 [D loss: 0.599360] [G loss: -0.396759]\n",
      "173 [D loss: 0.612081] [G loss: -0.564649]\n",
      "174 [D loss: 0.405396] [G loss: -0.593675]\n",
      "175 [D loss: 0.730151] [G loss: -0.764280]\n",
      "176 [D loss: 2.161072] [G loss: -0.603135]\n",
      "177 [D loss: 0.575330] [G loss: -0.522978]\n",
      "178 [D loss: 0.640279] [G loss: -0.502843]\n",
      "179 [D loss: 0.397632] [G loss: -0.581310]\n",
      "180 [D loss: 0.941901] [G loss: -0.604674]\n",
      "181 [D loss: 1.149541] [G loss: -0.643793]\n",
      "182 [D loss: 0.615126] [G loss: -0.641999]\n",
      "183 [D loss: 0.418777] [G loss: -0.541194]\n",
      "184 [D loss: 0.503061] [G loss: -0.603777]\n",
      "185 [D loss: 0.593089] [G loss: -0.620138]\n",
      "186 [D loss: 0.598378] [G loss: -0.577778]\n",
      "187 [D loss: 0.702492] [G loss: -0.556799]\n",
      "188 [D loss: 0.663449] [G loss: -0.690444]\n",
      "189 [D loss: 1.689828] [G loss: -0.685644]\n",
      "190 [D loss: 0.972585] [G loss: -0.534127]\n",
      "191 [D loss: 0.446636] [G loss: -0.509478]\n",
      "192 [D loss: 0.598162] [G loss: -0.516762]\n",
      "193 [D loss: 0.878424] [G loss: -0.603186]\n",
      "194 [D loss: 0.503230] [G loss: -0.527972]\n",
      "195 [D loss: 0.617772] [G loss: -0.545681]\n",
      "196 [D loss: 0.659797] [G loss: -0.587169]\n",
      "197 [D loss: 0.414720] [G loss: -0.536315]\n",
      "198 [D loss: 0.432567] [G loss: -0.479361]\n",
      "199 [D loss: 0.339912] [G loss: -0.526076]\n",
      "200 [D loss: 0.382577] [G loss: -0.477067]\n",
      "201 [D loss: 0.452044] [G loss: -0.476350]\n",
      "202 [D loss: 0.333833] [G loss: -0.548638]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "203 [D loss: 0.361302] [G loss: -0.513658]\n",
      "204 [D loss: 0.525469] [G loss: -0.525718]\n",
      "205 [D loss: 0.653579] [G loss: -0.581482]\n",
      "206 [D loss: 0.625306] [G loss: -0.436289]\n",
      "207 [D loss: 0.455320] [G loss: -0.632064]\n",
      "208 [D loss: 0.566715] [G loss: -0.398301]\n",
      "209 [D loss: 0.662943] [G loss: -0.342033]\n",
      "210 [D loss: 0.464181] [G loss: -0.524143]\n",
      "211 [D loss: 0.403231] [G loss: -0.507874]\n",
      "212 [D loss: 0.503835] [G loss: -0.538146]\n",
      "213 [D loss: 0.861545] [G loss: -0.479299]\n",
      "214 [D loss: 0.790792] [G loss: -0.666079]\n",
      "215 [D loss: 0.606473] [G loss: -0.570274]\n",
      "216 [D loss: 0.561009] [G loss: -0.557589]\n",
      "217 [D loss: 0.764969] [G loss: -0.443639]\n",
      "218 [D loss: 0.393532] [G loss: -0.481239]\n",
      "219 [D loss: 0.556012] [G loss: -0.538260]\n",
      "220 [D loss: 0.498577] [G loss: -0.564824]\n",
      "221 [D loss: 0.408642] [G loss: -0.472884]\n",
      "222 [D loss: 0.433498] [G loss: -0.529188]\n",
      "223 [D loss: 0.837080] [G loss: -0.649498]\n",
      "224 [D loss: 1.360623] [G loss: -0.520839]\n",
      "225 [D loss: 0.595602] [G loss: -0.525113]\n",
      "226 [D loss: 0.414051] [G loss: -0.452207]\n",
      "227 [D loss: 0.779339] [G loss: -0.672906]\n",
      "228 [D loss: 0.500150] [G loss: -0.563279]\n",
      "229 [D loss: 0.626894] [G loss: -0.467988]\n",
      "230 [D loss: 0.392756] [G loss: -0.503032]\n",
      "231 [D loss: 0.516379] [G loss: -0.645777]\n",
      "232 [D loss: 0.773514] [G loss: -0.451723]\n",
      "233 [D loss: 0.472578] [G loss: -0.597579]\n",
      "234 [D loss: 0.494931] [G loss: -0.565374]\n",
      "235 [D loss: 0.772290] [G loss: -0.528166]\n",
      "236 [D loss: 0.579271] [G loss: -0.460398]\n",
      "237 [D loss: 0.485951] [G loss: -0.583855]\n",
      "238 [D loss: 0.681355] [G loss: -0.581635]\n",
      "239 [D loss: 2.924965] [G loss: -0.613083]\n",
      "240 [D loss: 0.708089] [G loss: -0.528541]\n",
      "241 [D loss: 0.623277] [G loss: -0.678896]\n",
      "242 [D loss: 0.611048] [G loss: -0.588274]\n",
      "243 [D loss: 0.786828] [G loss: -0.659325]\n",
      "244 [D loss: 0.877082] [G loss: -0.576178]\n",
      "245 [D loss: 0.480597] [G loss: -0.568875]\n",
      "246 [D loss: 0.516005] [G loss: -0.568248]\n",
      "247 [D loss: 0.570204] [G loss: -0.665999]\n",
      "248 [D loss: 0.459131] [G loss: -0.532308]\n",
      "249 [D loss: 0.339588] [G loss: -0.560276]\n",
      "250 [D loss: 0.393662] [G loss: -0.623481]\n",
      "251 [D loss: 0.652877] [G loss: -0.554938]\n",
      "252 [D loss: 0.428306] [G loss: -0.541286]\n",
      "253 [D loss: 0.386870] [G loss: -0.599321]\n",
      "254 [D loss: 0.417850] [G loss: -0.527530]\n",
      "255 [D loss: 0.507126] [G loss: -0.511206]\n",
      "256 [D loss: 0.709330] [G loss: -0.441021]\n",
      "257 [D loss: 1.228596] [G loss: -0.670149]\n",
      "258 [D loss: 0.467649] [G loss: -0.601373]\n",
      "259 [D loss: 0.477285] [G loss: -0.572546]\n",
      "260 [D loss: 0.543316] [G loss: -0.616771]\n",
      "261 [D loss: 0.529848] [G loss: -0.613923]\n",
      "262 [D loss: 0.467837] [G loss: -0.553593]\n",
      "263 [D loss: 0.347903] [G loss: -0.573920]\n",
      "264 [D loss: 0.433294] [G loss: -0.536736]\n",
      "265 [D loss: 0.296229] [G loss: -0.442396]\n",
      "266 [D loss: 0.598583] [G loss: -0.426518]\n",
      "267 [D loss: 0.995127] [G loss: -0.512484]\n",
      "268 [D loss: 0.486796] [G loss: -0.498358]\n",
      "269 [D loss: 0.500132] [G loss: -0.534527]\n",
      "270 [D loss: 0.483100] [G loss: -0.576255]\n",
      "271 [D loss: 0.441586] [G loss: -0.530898]\n",
      "272 [D loss: 0.306155] [G loss: -0.514589]\n",
      "273 [D loss: 0.445469] [G loss: -0.539574]\n",
      "274 [D loss: 0.362601] [G loss: -0.500821]\n",
      "275 [D loss: 0.381981] [G loss: -0.582833]\n",
      "276 [D loss: 0.421183] [G loss: -0.463457]\n",
      "277 [D loss: 0.604481] [G loss: -0.511844]\n",
      "278 [D loss: 0.424475] [G loss: -0.531695]\n",
      "279 [D loss: 0.794741] [G loss: -0.611510]\n",
      "280 [D loss: 0.678205] [G loss: -0.546023]\n",
      "281 [D loss: 0.617188] [G loss: -0.555232]\n",
      "282 [D loss: 0.615198] [G loss: -0.530173]\n",
      "283 [D loss: 0.494108] [G loss: -0.595884]\n",
      "284 [D loss: 0.364059] [G loss: -0.481427]\n",
      "285 [D loss: 0.348901] [G loss: -0.474430]\n",
      "286 [D loss: 0.448861] [G loss: -0.544536]\n",
      "287 [D loss: 0.383406] [G loss: -0.555514]\n",
      "288 [D loss: 0.693374] [G loss: -0.514431]\n",
      "289 [D loss: 1.095204] [G loss: -0.559435]\n",
      "290 [D loss: 0.364003] [G loss: -0.499699]\n",
      "291 [D loss: 0.250633] [G loss: -0.483132]\n",
      "292 [D loss: 0.400750] [G loss: -0.345406]\n",
      "293 [D loss: 0.381052] [G loss: -0.360314]\n",
      "294 [D loss: 0.502364] [G loss: -0.488639]\n",
      "295 [D loss: 0.913574] [G loss: -0.432606]\n",
      "296 [D loss: 1.882932] [G loss: -0.621193]\n",
      "297 [D loss: 0.581901] [G loss: -0.502455]\n",
      "298 [D loss: 0.856960] [G loss: -0.485467]\n",
      "299 [D loss: 1.513038] [G loss: -0.355713]\n",
      "300 [D loss: 0.344626] [G loss: -0.431933]\n",
      "301 [D loss: 0.543560] [G loss: -0.444620]\n",
      "302 [D loss: 0.650387] [G loss: -0.361309]\n",
      "303 [D loss: 0.641008] [G loss: -0.515789]\n",
      "304 [D loss: 0.804883] [G loss: -0.475894]\n",
      "305 [D loss: 0.716454] [G loss: -0.535088]\n",
      "306 [D loss: 0.407369] [G loss: -0.510755]\n",
      "307 [D loss: 0.532587] [G loss: -0.547667]\n",
      "308 [D loss: 0.488218] [G loss: -0.546699]\n",
      "309 [D loss: 0.615585] [G loss: -0.592445]\n",
      "310 [D loss: 0.488051] [G loss: -0.551647]\n",
      "311 [D loss: 0.420116] [G loss: -0.567151]\n",
      "312 [D loss: 0.488473] [G loss: -0.533839]\n",
      "313 [D loss: 0.345472] [G loss: -0.437237]\n",
      "314 [D loss: 0.555507] [G loss: -0.516220]\n",
      "315 [D loss: 0.538922] [G loss: -0.435606]\n",
      "316 [D loss: 0.254151] [G loss: -0.434676]\n",
      "317 [D loss: 0.386227] [G loss: -0.363508]\n",
      "318 [D loss: 0.334231] [G loss: -0.392741]\n",
      "319 [D loss: 0.520837] [G loss: -0.558774]\n",
      "320 [D loss: 1.982515] [G loss: -0.615353]\n",
      "321 [D loss: 1.800586] [G loss: -0.366907]\n",
      "322 [D loss: 0.782538] [G loss: -0.541925]\n",
      "323 [D loss: 0.519379] [G loss: -0.574571]\n",
      "324 [D loss: 0.604007] [G loss: -0.511738]\n",
      "325 [D loss: 0.420077] [G loss: -0.539220]\n",
      "326 [D loss: 0.457874] [G loss: -0.535519]\n",
      "327 [D loss: 0.441427] [G loss: -0.579121]\n",
      "328 [D loss: 0.385974] [G loss: -0.533819]\n",
      "329 [D loss: 0.500965] [G loss: -0.541721]\n",
      "330 [D loss: 0.385733] [G loss: -0.434072]\n",
      "331 [D loss: 0.445063] [G loss: -0.408895]\n",
      "332 [D loss: 0.355171] [G loss: -0.492331]\n",
      "333 [D loss: 0.918176] [G loss: -0.347942]\n",
      "334 [D loss: 1.741085] [G loss: -0.584711]\n",
      "335 [D loss: 0.749629] [G loss: -0.480702]\n",
      "336 [D loss: 0.895603] [G loss: -0.357745]\n",
      "337 [D loss: 0.694380] [G loss: -0.454588]\n",
      "338 [D loss: 0.463529] [G loss: -0.508889]\n",
      "339 [D loss: 0.481580] [G loss: -0.509084]\n",
      "340 [D loss: 1.067412] [G loss: -0.569309]\n",
      "341 [D loss: 1.213070] [G loss: -0.575533]\n",
      "342 [D loss: 1.648111] [G loss: -0.313368]\n",
      "343 [D loss: 0.969416] [G loss: -0.429052]\n",
      "344 [D loss: 0.504193] [G loss: -0.548252]\n",
      "345 [D loss: 0.795756] [G loss: -0.646993]\n",
      "346 [D loss: 0.919317] [G loss: -0.646516]\n",
      "347 [D loss: 0.815536] [G loss: -0.583817]\n",
      "348 [D loss: 0.502567] [G loss: -0.589517]\n",
      "349 [D loss: 0.614235] [G loss: -0.577875]\n",
      "350 [D loss: 0.656463] [G loss: -0.622247]\n",
      "351 [D loss: 0.757081] [G loss: -0.517966]\n",
      "352 [D loss: 0.670289] [G loss: -0.624913]\n",
      "353 [D loss: 0.467897] [G loss: -0.593148]\n",
      "354 [D loss: 0.461218] [G loss: -0.469108]\n",
      "355 [D loss: 0.570245] [G loss: -0.460537]\n",
      "356 [D loss: 0.463737] [G loss: -0.384631]\n",
      "357 [D loss: 0.680540] [G loss: -0.482220]\n",
      "358 [D loss: 0.517544] [G loss: -0.512714]\n",
      "359 [D loss: 0.977596] [G loss: -0.396759]\n",
      "360 [D loss: 1.309645] [G loss: -0.599285]\n",
      "361 [D loss: 1.032056] [G loss: -0.532013]\n",
      "362 [D loss: 1.307201] [G loss: -0.370911]\n",
      "363 [D loss: 1.105405] [G loss: -0.533899]\n",
      "364 [D loss: 0.641021] [G loss: -0.619419]\n",
      "365 [D loss: 0.739724] [G loss: -0.614248]\n",
      "366 [D loss: 0.509455] [G loss: -0.536819]\n",
      "367 [D loss: 0.656579] [G loss: -0.544289]\n",
      "368 [D loss: 0.715092] [G loss: -0.533909]\n",
      "369 [D loss: 1.087631] [G loss: -0.483508]\n",
      "370 [D loss: 0.792486] [G loss: -0.584592]\n",
      "371 [D loss: 0.539544] [G loss: -0.643635]\n",
      "372 [D loss: 0.687985] [G loss: -0.593075]\n",
      "373 [D loss: 0.869933] [G loss: -0.506765]\n",
      "374 [D loss: 0.854730] [G loss: -0.662370]\n",
      "375 [D loss: 0.719906] [G loss: -0.661060]\n",
      "376 [D loss: 0.591585] [G loss: -0.632434]\n",
      "377 [D loss: 0.673845] [G loss: -0.507338]\n",
      "378 [D loss: 0.525068] [G loss: -0.512275]\n",
      "379 [D loss: 0.700561] [G loss: -0.504209]\n",
      "380 [D loss: 0.654275] [G loss: -0.488509]\n",
      "381 [D loss: 0.750511] [G loss: -0.579191]\n",
      "382 [D loss: 0.605928] [G loss: -0.581855]\n",
      "383 [D loss: 0.691111] [G loss: -0.555426]\n",
      "384 [D loss: 0.393046] [G loss: -0.564045]\n",
      "385 [D loss: 0.481949] [G loss: -0.589339]\n",
      "386 [D loss: 0.432449] [G loss: -0.618067]\n",
      "387 [D loss: 1.041923] [G loss: -0.487708]\n",
      "388 [D loss: 0.642125] [G loss: -0.325392]\n",
      "389 [D loss: 2.099889] [G loss: -0.640675]\n",
      "390 [D loss: 0.645487] [G loss: -0.622630]\n",
      "391 [D loss: 0.695240] [G loss: -0.568735]\n",
      "392 [D loss: 0.780635] [G loss: -0.644016]\n",
      "393 [D loss: 0.618969] [G loss: -0.528730]\n",
      "394 [D loss: 0.483063] [G loss: -0.539005]\n",
      "395 [D loss: 0.772745] [G loss: -0.529268]\n",
      "396 [D loss: 0.724788] [G loss: -0.483165]\n",
      "397 [D loss: 0.392268] [G loss: -0.510593]\n",
      "398 [D loss: 0.533035] [G loss: -0.562978]\n",
      "399 [D loss: 0.554066] [G loss: -0.519871]\n",
      "400 [D loss: 0.868713] [G loss: -0.599811]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12563887\n",
      "0.4658884\n",
      "0.13303712\n",
      "0.48543835\n",
      "0.13291276\n",
      "0.4859477\n",
      "0.1300203\n",
      "0.460255\n",
      "0.13083398\n",
      "0.46880206\n",
      "0.13185136\n",
      "0.48805174\n",
      "0.12853073\n",
      "0.53200203\n",
      "0.13354595\n",
      "0.49658525\n",
      "0.12523422\n",
      "0.49074027\n",
      "0.12719807\n",
      "0.46517086\n",
      "0.1313209\n",
      "0.4660835\n",
      "0.12783927\n",
      "0.4809845\n",
      "0.13076809\n",
      "0.47664934\n",
      "0.12915312\n",
      "0.49182612\n",
      "0.13075061\n",
      "0.5234787\n",
      "0.12777582\n",
      "0.4901301\n",
      "0.1286911\n",
      "0.47122747\n",
      "0.12570018\n",
      "0.4862102\n",
      "0.13329057\n",
      "0.46647403\n",
      "0.13073394\n",
      "0.46809793\n",
      "0.13103133\n",
      "0.5185693\n",
      "0.12803188\n",
      "0.4745729\n",
      "0.12661597\n",
      "0.5107792\n",
      "0.12722406\n",
      "0.4976506\n",
      "0.12512995\n",
      "0.4947844\n",
      "401 [D loss: 0.822134] [G loss: -0.471544]\n",
      "402 [D loss: 0.882148] [G loss: -0.371779]\n",
      "403 [D loss: 0.640644] [G loss: -0.527206]\n",
      "404 [D loss: 0.562593] [G loss: -0.627973]\n",
      "405 [D loss: 0.908801] [G loss: -0.482755]\n",
      "406 [D loss: 0.626756] [G loss: -0.495470]\n",
      "407 [D loss: 0.863414] [G loss: -0.541699]\n",
      "408 [D loss: 0.613458] [G loss: -0.566728]\n",
      "409 [D loss: 0.426686] [G loss: -0.516137]\n",
      "410 [D loss: 0.491689] [G loss: -0.557128]\n",
      "411 [D loss: 0.400097] [G loss: -0.537657]\n",
      "412 [D loss: 0.508600] [G loss: -0.498750]\n",
      "413 [D loss: 0.354089] [G loss: -0.419046]\n",
      "414 [D loss: 0.502866] [G loss: -0.592552]\n",
      "415 [D loss: 0.584324] [G loss: -0.593292]\n",
      "416 [D loss: 0.534779] [G loss: -0.606355]\n",
      "417 [D loss: 0.958983] [G loss: -0.598002]\n",
      "418 [D loss: 0.591039] [G loss: -0.692292]\n",
      "419 [D loss: 0.629828] [G loss: -0.471940]\n",
      "420 [D loss: 0.732118] [G loss: -0.521209]\n",
      "421 [D loss: 0.728023] [G loss: -0.481937]\n",
      "422 [D loss: 1.126220] [G loss: -0.371626]\n",
      "423 [D loss: 3.498224] [G loss: -0.663006]\n",
      "424 [D loss: 0.618304] [G loss: -0.654281]\n",
      "425 [D loss: 0.720687] [G loss: -0.720967]\n",
      "426 [D loss: 0.643086] [G loss: -0.702779]\n",
      "427 [D loss: 0.963145] [G loss: -0.654108]\n",
      "428 [D loss: 0.534724] [G loss: -0.523064]\n",
      "429 [D loss: 0.725502] [G loss: -0.565747]\n",
      "430 [D loss: 0.598291] [G loss: -0.562004]\n",
      "431 [D loss: 0.559651] [G loss: -0.581546]\n",
      "432 [D loss: 0.725286] [G loss: -0.525906]\n",
      "433 [D loss: 0.396775] [G loss: -0.571107]\n",
      "434 [D loss: 0.474368] [G loss: -0.460815]\n",
      "435 [D loss: 0.610219] [G loss: -0.583509]\n",
      "436 [D loss: 0.580046] [G loss: -0.506061]\n",
      "437 [D loss: 0.563623] [G loss: -0.455864]\n",
      "438 [D loss: 1.976908] [G loss: -0.529238]\n",
      "439 [D loss: 0.837253] [G loss: -0.583937]\n",
      "440 [D loss: 0.591509] [G loss: -0.646294]\n",
      "441 [D loss: 0.635895] [G loss: -0.528289]\n",
      "442 [D loss: 0.490262] [G loss: -0.455164]\n",
      "443 [D loss: 0.371878] [G loss: -0.537856]\n",
      "444 [D loss: 0.510737] [G loss: -0.500843]\n",
      "445 [D loss: 0.420179] [G loss: -0.492000]\n",
      "446 [D loss: 0.611588] [G loss: -0.469138]\n",
      "447 [D loss: 0.484455] [G loss: -0.577149]\n",
      "448 [D loss: 0.555229] [G loss: -0.655729]\n",
      "449 [D loss: 0.563493] [G loss: -0.606056]\n",
      "450 [D loss: 0.382390] [G loss: -0.556794]\n",
      "451 [D loss: 0.666594] [G loss: -0.545957]\n",
      "452 [D loss: 0.830002] [G loss: -0.499844]\n",
      "453 [D loss: 0.840610] [G loss: -0.511812]\n",
      "454 [D loss: 0.438614] [G loss: -0.570642]\n",
      "455 [D loss: 0.424104] [G loss: -0.462816]\n",
      "456 [D loss: 0.664413] [G loss: -0.521463]\n",
      "457 [D loss: 0.827258] [G loss: -0.418831]\n",
      "458 [D loss: 0.379435] [G loss: -0.334277]\n",
      "459 [D loss: 0.407949] [G loss: -0.392671]\n",
      "460 [D loss: 0.305509] [G loss: -0.321947]\n",
      "461 [D loss: 1.186316] [G loss: -0.525459]\n",
      "462 [D loss: 0.525412] [G loss: -0.576955]\n",
      "463 [D loss: 0.920008] [G loss: -0.574769]\n",
      "464 [D loss: 1.127087] [G loss: -0.563540]\n",
      "465 [D loss: 0.685137] [G loss: -0.567414]\n",
      "466 [D loss: 0.552336] [G loss: -0.616844]\n",
      "467 [D loss: 0.481568] [G loss: -0.612085]\n",
      "468 [D loss: 0.528139] [G loss: -0.611796]\n",
      "469 [D loss: 0.556704] [G loss: -0.548356]\n",
      "470 [D loss: 0.565477] [G loss: -0.638798]\n",
      "471 [D loss: 0.578228] [G loss: -0.634740]\n",
      "472 [D loss: 0.794469] [G loss: -0.672636]\n",
      "473 [D loss: 0.828089] [G loss: -0.570915]\n",
      "474 [D loss: 0.378443] [G loss: -0.543940]\n",
      "475 [D loss: 0.499863] [G loss: -0.566726]\n",
      "476 [D loss: 0.486354] [G loss: -0.500318]\n",
      "477 [D loss: 0.344393] [G loss: -0.542620]\n",
      "478 [D loss: 0.341015] [G loss: -0.500392]\n",
      "479 [D loss: 0.648098] [G loss: -0.472978]\n",
      "480 [D loss: 0.636596] [G loss: -0.567434]\n",
      "481 [D loss: 0.542342] [G loss: -0.577977]\n",
      "482 [D loss: 0.554186] [G loss: -0.599828]\n",
      "483 [D loss: 0.872324] [G loss: -0.574552]\n",
      "484 [D loss: 1.052669] [G loss: -0.654039]\n",
      "485 [D loss: 1.197673] [G loss: -0.595626]\n",
      "486 [D loss: 0.849182] [G loss: -0.574709]\n",
      "487 [D loss: 1.590777] [G loss: -0.686329]\n",
      "488 [D loss: 0.619766] [G loss: -0.704377]\n",
      "489 [D loss: 0.885340] [G loss: -0.570394]\n",
      "490 [D loss: 1.286709] [G loss: -0.643495]\n",
      "491 [D loss: 0.565896] [G loss: -0.652702]\n",
      "492 [D loss: 0.508983] [G loss: -0.600366]\n",
      "493 [D loss: 0.670294] [G loss: -0.581874]\n",
      "494 [D loss: 1.943739] [G loss: -0.659479]\n",
      "495 [D loss: 0.555977] [G loss: -0.700437]\n",
      "496 [D loss: 0.734445] [G loss: -0.576668]\n",
      "497 [D loss: 0.566196] [G loss: -0.714448]\n",
      "498 [D loss: 0.674469] [G loss: -0.614402]\n",
      "499 [D loss: 0.621399] [G loss: -0.593395]\n",
      "500 [D loss: 0.505131] [G loss: -0.681800]\n",
      "501 [D loss: 0.628722] [G loss: -0.521285]\n",
      "502 [D loss: 0.697944] [G loss: -0.528848]\n",
      "503 [D loss: 0.645575] [G loss: -0.521533]\n",
      "504 [D loss: 0.843740] [G loss: -0.442795]\n",
      "505 [D loss: 1.214160] [G loss: -0.665371]\n",
      "506 [D loss: 0.957886] [G loss: -0.721289]\n",
      "507 [D loss: 0.925979] [G loss: -0.713290]\n",
      "508 [D loss: 0.664585] [G loss: -0.622308]\n",
      "509 [D loss: 0.735036] [G loss: -0.572547]\n",
      "510 [D loss: 1.094154] [G loss: -0.625126]\n",
      "511 [D loss: 0.800703] [G loss: -0.628398]\n",
      "512 [D loss: 1.054889] [G loss: -0.562160]\n",
      "513 [D loss: 0.932188] [G loss: -0.587481]\n",
      "514 [D loss: 0.688450] [G loss: -0.453873]\n",
      "515 [D loss: 0.522954] [G loss: -0.438625]\n",
      "516 [D loss: 0.470258] [G loss: -0.423363]\n",
      "517 [D loss: 0.479877] [G loss: -0.546526]\n",
      "518 [D loss: 0.595829] [G loss: -0.396200]\n",
      "519 [D loss: 0.754212] [G loss: -0.467853]\n",
      "520 [D loss: 1.364377] [G loss: -0.335369]\n",
      "521 [D loss: 0.525940] [G loss: -0.519394]\n",
      "522 [D loss: 0.525623] [G loss: -0.595073]\n",
      "523 [D loss: 0.814312] [G loss: -0.439141]\n",
      "524 [D loss: 2.214194] [G loss: -0.727421]\n",
      "525 [D loss: 0.950913] [G loss: -0.618361]\n",
      "526 [D loss: 1.263483] [G loss: -0.462642]\n",
      "527 [D loss: 0.996637] [G loss: -0.731414]\n",
      "528 [D loss: 0.854370] [G loss: -0.688171]\n",
      "529 [D loss: 0.680333] [G loss: -0.684155]\n",
      "530 [D loss: 0.536091] [G loss: -0.650200]\n",
      "531 [D loss: 0.699167] [G loss: -0.762019]\n",
      "532 [D loss: 0.637819] [G loss: -0.718535]\n",
      "533 [D loss: 0.997066] [G loss: -0.636962]\n",
      "534 [D loss: 0.763646] [G loss: -0.454163]\n",
      "535 [D loss: 0.530743] [G loss: -0.408408]\n",
      "536 [D loss: 0.500606] [G loss: -0.488452]\n",
      "537 [D loss: 0.465583] [G loss: -0.507885]\n",
      "538 [D loss: 0.574737] [G loss: -0.628463]\n",
      "539 [D loss: 0.481528] [G loss: -0.647239]\n",
      "540 [D loss: 0.642103] [G loss: -0.684104]\n",
      "541 [D loss: 0.680305] [G loss: -0.572652]\n",
      "542 [D loss: 0.695922] [G loss: -0.521868]\n",
      "543 [D loss: 0.519309] [G loss: -0.394669]\n",
      "544 [D loss: 0.865422] [G loss: -0.417504]\n",
      "545 [D loss: 1.159397] [G loss: -0.575883]\n",
      "546 [D loss: 0.984628] [G loss: -0.684629]\n",
      "547 [D loss: 0.711163] [G loss: -0.597021]\n",
      "548 [D loss: 1.125876] [G loss: -0.642258]\n",
      "549 [D loss: 0.584269] [G loss: -0.563222]\n",
      "550 [D loss: 0.508106] [G loss: -0.602762]\n",
      "551 [D loss: 0.576735] [G loss: -0.540995]\n",
      "552 [D loss: 0.670153] [G loss: -0.408619]\n",
      "553 [D loss: 0.503892] [G loss: -0.413240]\n",
      "554 [D loss: 0.318180] [G loss: -0.289174]\n",
      "555 [D loss: 0.435665] [G loss: -0.338111]\n",
      "556 [D loss: 0.437435] [G loss: -0.329863]\n",
      "557 [D loss: 0.456477] [G loss: -0.442300]\n",
      "558 [D loss: 0.411576] [G loss: -0.486896]\n",
      "559 [D loss: 0.401859] [G loss: -0.242776]\n",
      "560 [D loss: 0.845149] [G loss: -0.528065]\n",
      "561 [D loss: 0.755056] [G loss: -0.428567]\n",
      "562 [D loss: 0.677035] [G loss: -0.426602]\n",
      "563 [D loss: 1.892000] [G loss: -0.654745]\n",
      "564 [D loss: 0.897429] [G loss: -0.683780]\n",
      "565 [D loss: 0.729087] [G loss: -0.661351]\n",
      "566 [D loss: 0.881761] [G loss: -0.658416]\n",
      "567 [D loss: 0.790309] [G loss: -0.642731]\n",
      "568 [D loss: 0.652618] [G loss: -0.606936]\n",
      "569 [D loss: 0.587686] [G loss: -0.524787]\n",
      "570 [D loss: 0.880448] [G loss: -0.673038]\n",
      "571 [D loss: 0.727681] [G loss: -0.655812]\n",
      "572 [D loss: 0.831317] [G loss: -0.527974]\n",
      "573 [D loss: 1.021968] [G loss: -0.786156]\n",
      "574 [D loss: 1.101951] [G loss: -0.738655]\n",
      "575 [D loss: 1.371441] [G loss: -0.625645]\n",
      "576 [D loss: 1.029941] [G loss: -0.713844]\n",
      "577 [D loss: 1.069593] [G loss: -0.774619]\n",
      "578 [D loss: 0.607838] [G loss: -0.690385]\n",
      "579 [D loss: 0.822830] [G loss: -0.751208]\n",
      "580 [D loss: 0.848158] [G loss: -0.635733]\n",
      "581 [D loss: 0.710658] [G loss: -0.669054]\n",
      "582 [D loss: 1.193754] [G loss: -0.539838]\n",
      "583 [D loss: 0.908699] [G loss: -0.425303]\n",
      "584 [D loss: 0.598446] [G loss: -0.499437]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "585 [D loss: 0.873135] [G loss: -0.596078]\n",
      "586 [D loss: 0.442985] [G loss: -0.609748]\n",
      "587 [D loss: 0.882016] [G loss: -0.723651]\n",
      "588 [D loss: 0.792192] [G loss: -0.674308]\n",
      "589 [D loss: 0.670948] [G loss: -0.603968]\n",
      "590 [D loss: 0.840340] [G loss: -0.521121]\n",
      "591 [D loss: 0.610503] [G loss: -0.590759]\n",
      "592 [D loss: 0.600546] [G loss: -0.549647]\n",
      "593 [D loss: 0.743008] [G loss: -0.505479]\n",
      "594 [D loss: 0.557540] [G loss: -0.456892]\n",
      "595 [D loss: 0.540676] [G loss: -0.466437]\n",
      "596 [D loss: 0.657834] [G loss: -0.516971]\n",
      "597 [D loss: 0.927840] [G loss: -0.567397]\n",
      "598 [D loss: 0.824045] [G loss: -0.537347]\n",
      "599 [D loss: 0.546667] [G loss: -0.594468]\n",
      "600 [D loss: 0.842587] [G loss: -0.542214]\n",
      "601 [D loss: 2.038980] [G loss: -0.755586]\n",
      "602 [D loss: 1.056968] [G loss: -0.703628]\n",
      "603 [D loss: 0.885473] [G loss: -0.693849]\n",
      "604 [D loss: 1.965349] [G loss: -0.705020]\n",
      "605 [D loss: 1.039681] [G loss: -0.672767]\n",
      "606 [D loss: 1.136731] [G loss: -0.589937]\n",
      "607 [D loss: 0.504906] [G loss: -0.553180]\n",
      "608 [D loss: 0.887160] [G loss: -0.734038]\n",
      "609 [D loss: 0.712262] [G loss: -0.755730]\n",
      "610 [D loss: 0.693149] [G loss: -0.714013]\n",
      "611 [D loss: 0.892014] [G loss: -0.585867]\n",
      "612 [D loss: 0.574966] [G loss: -0.511129]\n",
      "613 [D loss: 1.390389] [G loss: -0.758512]\n",
      "614 [D loss: 1.955395] [G loss: -0.594179]\n",
      "615 [D loss: 1.597079] [G loss: -0.476783]\n",
      "616 [D loss: 3.233517] [G loss: -0.735924]\n",
      "617 [D loss: 0.917691] [G loss: -0.765807]\n",
      "618 [D loss: 0.637375] [G loss: -0.812622]\n",
      "619 [D loss: 0.769572] [G loss: -0.828590]\n",
      "620 [D loss: 0.640037] [G loss: -0.828382]\n",
      "621 [D loss: 0.771030] [G loss: -0.813731]\n",
      "622 [D loss: 0.726955] [G loss: -0.727731]\n",
      "623 [D loss: 1.003057] [G loss: -0.686974]\n",
      "624 [D loss: 0.843776] [G loss: -0.746305]\n",
      "625 [D loss: 1.171344] [G loss: -0.735804]\n",
      "626 [D loss: 0.713347] [G loss: -0.668355]\n",
      "627 [D loss: 0.530952] [G loss: -0.575272]\n",
      "628 [D loss: 0.964501] [G loss: -0.650988]\n",
      "629 [D loss: 0.505586] [G loss: -0.590559]\n",
      "630 [D loss: 0.569629] [G loss: -0.524083]\n",
      "631 [D loss: 0.712630] [G loss: -0.478284]\n",
      "632 [D loss: 1.932852] [G loss: -0.744696]\n",
      "633 [D loss: 0.890332] [G loss: -0.558236]\n",
      "634 [D loss: 0.621579] [G loss: -0.507782]\n",
      "635 [D loss: 1.312745] [G loss: -0.654386]\n",
      "636 [D loss: 2.189801] [G loss: -0.769059]\n",
      "637 [D loss: 0.771771] [G loss: -0.830122]\n",
      "638 [D loss: 0.745612] [G loss: -0.859692]\n",
      "639 [D loss: 0.926031] [G loss: -0.849026]\n",
      "640 [D loss: 0.681770] [G loss: -0.859637]\n",
      "641 [D loss: 0.648064] [G loss: -0.797238]\n",
      "642 [D loss: 1.112485] [G loss: -0.742987]\n",
      "643 [D loss: 0.825859] [G loss: -0.710357]\n",
      "644 [D loss: 0.742993] [G loss: -0.613501]\n",
      "645 [D loss: 1.203343] [G loss: -0.662540]\n",
      "646 [D loss: 0.695824] [G loss: -0.663276]\n",
      "647 [D loss: 1.392636] [G loss: -0.629088]\n",
      "648 [D loss: 2.417142] [G loss: -0.763406]\n",
      "649 [D loss: 1.596482] [G loss: -0.796835]\n",
      "650 [D loss: 1.459446] [G loss: -0.739098]\n",
      "651 [D loss: 2.462548] [G loss: -0.861516]\n",
      "652 [D loss: 0.931854] [G loss: -0.890301]\n",
      "653 [D loss: 0.794735] [G loss: -0.913431]\n",
      "654 [D loss: 0.854418] [G loss: -0.909700]\n",
      "655 [D loss: 0.866422] [G loss: -0.852209]\n",
      "656 [D loss: 1.125140] [G loss: -0.801641]\n",
      "657 [D loss: 0.598591] [G loss: -0.683924]\n",
      "658 [D loss: 0.586044] [G loss: -0.616938]\n",
      "659 [D loss: 0.615658] [G loss: -0.554121]\n",
      "660 [D loss: 0.592287] [G loss: -0.568900]\n",
      "661 [D loss: 0.607504] [G loss: -0.490793]\n",
      "662 [D loss: 0.984104] [G loss: -0.508435]\n",
      "663 [D loss: 0.670968] [G loss: -0.481170]\n",
      "664 [D loss: 0.583990] [G loss: -0.452560]\n",
      "665 [D loss: 0.508355] [G loss: -0.512920]\n",
      "666 [D loss: 0.699334] [G loss: -0.575736]\n",
      "667 [D loss: 0.938312] [G loss: -0.639946]\n",
      "668 [D loss: 0.695931] [G loss: -0.524712]\n",
      "669 [D loss: 0.762756] [G loss: -0.571905]\n",
      "670 [D loss: 0.669001] [G loss: -0.684517]\n",
      "671 [D loss: 1.871699] [G loss: -0.459389]\n",
      "672 [D loss: 0.652435] [G loss: -0.455173]\n",
      "673 [D loss: 0.873032] [G loss: -0.493144]\n",
      "674 [D loss: 0.760010] [G loss: -0.661651]\n",
      "675 [D loss: 0.647072] [G loss: -0.703307]\n",
      "676 [D loss: 0.612419] [G loss: -0.551956]\n",
      "677 [D loss: 0.566580] [G loss: -0.483301]\n",
      "678 [D loss: 0.684520] [G loss: -0.514667]\n",
      "679 [D loss: 0.858146] [G loss: -0.662029]\n",
      "680 [D loss: 1.234578] [G loss: -0.766102]\n",
      "681 [D loss: 1.061458] [G loss: -0.769634]\n",
      "682 [D loss: 0.826439] [G loss: -0.474221]\n",
      "683 [D loss: 0.712802] [G loss: -0.300624]\n",
      "684 [D loss: 0.811991] [G loss: -0.510784]\n",
      "685 [D loss: 0.533392] [G loss: -0.546823]\n",
      "686 [D loss: 0.677189] [G loss: -0.608714]\n",
      "687 [D loss: 0.588145] [G loss: -0.577510]\n",
      "688 [D loss: 0.602273] [G loss: -0.721883]\n",
      "689 [D loss: 0.604274] [G loss: -0.634221]\n",
      "690 [D loss: 0.857641] [G loss: -0.594180]\n",
      "691 [D loss: 0.542620] [G loss: -0.493873]\n",
      "692 [D loss: 1.016649] [G loss: -0.543788]\n",
      "693 [D loss: 1.423064] [G loss: -0.602274]\n",
      "694 [D loss: 1.125134] [G loss: -0.720781]\n",
      "695 [D loss: 1.370300] [G loss: -0.784797]\n",
      "696 [D loss: 3.546220] [G loss: -0.834735]\n",
      "697 [D loss: 1.156700] [G loss: -0.815808]\n",
      "698 [D loss: 0.743500] [G loss: -0.833433]\n",
      "699 [D loss: 0.909530] [G loss: -0.939299]\n",
      "700 [D loss: 1.657390] [G loss: -0.919140]\n",
      "701 [D loss: 1.194797] [G loss: -0.758089]\n",
      "702 [D loss: 0.636807] [G loss: -0.692454]\n",
      "703 [D loss: 0.729157] [G loss: -0.720541]\n",
      "704 [D loss: 0.726949] [G loss: -0.784979]\n",
      "705 [D loss: 0.627371] [G loss: -0.822473]\n",
      "706 [D loss: 0.816301] [G loss: -0.817267]\n",
      "707 [D loss: 0.738091] [G loss: -0.789818]\n",
      "708 [D loss: 1.109557] [G loss: -0.691932]\n",
      "709 [D loss: 0.917654] [G loss: -0.849607]\n",
      "710 [D loss: 0.847765] [G loss: -0.882059]\n",
      "711 [D loss: 0.606508] [G loss: -0.798670]\n",
      "712 [D loss: 0.641355] [G loss: -0.801892]\n",
      "713 [D loss: 0.655349] [G loss: -0.527457]\n",
      "714 [D loss: 0.940347] [G loss: -0.612789]\n",
      "715 [D loss: 1.502972] [G loss: -0.831884]\n",
      "716 [D loss: 0.650074] [G loss: -0.774904]\n",
      "717 [D loss: 0.565369] [G loss: -0.578587]\n",
      "718 [D loss: 0.909802] [G loss: -0.611903]\n",
      "719 [D loss: 0.762944] [G loss: -0.406779]\n",
      "720 [D loss: 0.919450] [G loss: -0.520884]\n",
      "721 [D loss: 0.504044] [G loss: -0.440923]\n",
      "722 [D loss: 0.563542] [G loss: -0.472952]\n",
      "723 [D loss: 0.514645] [G loss: -0.490217]\n",
      "724 [D loss: 0.627867] [G loss: -0.543110]\n",
      "725 [D loss: 0.536844] [G loss: -0.548986]\n",
      "726 [D loss: 0.884912] [G loss: -0.586071]\n",
      "727 [D loss: 0.747283] [G loss: -0.600436]\n",
      "728 [D loss: 1.136519] [G loss: -0.617267]\n",
      "729 [D loss: 1.598602] [G loss: -0.669572]\n",
      "730 [D loss: 1.200238] [G loss: -0.611412]\n",
      "731 [D loss: 0.623539] [G loss: -0.519129]\n",
      "732 [D loss: 0.550705] [G loss: -0.499563]\n",
      "733 [D loss: 1.216463] [G loss: -0.642247]\n",
      "734 [D loss: 1.055356] [G loss: -0.711905]\n",
      "735 [D loss: 1.579942] [G loss: -0.822271]\n",
      "736 [D loss: 1.179490] [G loss: -0.842195]\n",
      "737 [D loss: 1.445357] [G loss: -0.830354]\n",
      "738 [D loss: 0.702386] [G loss: -0.785005]\n",
      "739 [D loss: 0.907138] [G loss: -0.724778]\n",
      "740 [D loss: 0.810937] [G loss: -0.823972]\n",
      "741 [D loss: 0.821281] [G loss: -0.807762]\n",
      "742 [D loss: 1.118613] [G loss: -0.709018]\n",
      "743 [D loss: 0.725302] [G loss: -0.572531]\n",
      "744 [D loss: 0.665636] [G loss: -0.450834]\n",
      "745 [D loss: 0.602315] [G loss: -0.449660]\n",
      "746 [D loss: 0.617085] [G loss: -0.472036]\n",
      "747 [D loss: 0.907522] [G loss: -0.512232]\n",
      "748 [D loss: 0.969855] [G loss: -0.396117]\n",
      "749 [D loss: 1.264134] [G loss: -0.435581]\n",
      "750 [D loss: 0.999032] [G loss: -0.520847]\n",
      "751 [D loss: 0.898061] [G loss: -0.526147]\n",
      "752 [D loss: 1.540052] [G loss: -0.696068]\n",
      "753 [D loss: 0.840429] [G loss: -0.746242]\n",
      "754 [D loss: 0.886565] [G loss: -0.643860]\n",
      "755 [D loss: 0.643155] [G loss: -0.663224]\n",
      "756 [D loss: 0.704438] [G loss: -0.651471]\n",
      "757 [D loss: 0.746828] [G loss: -0.701102]\n",
      "758 [D loss: 0.815132] [G loss: -0.754694]\n",
      "759 [D loss: 0.848375] [G loss: -0.782803]\n",
      "760 [D loss: 0.682309] [G loss: -0.506786]\n",
      "761 [D loss: 0.690349] [G loss: -0.395818]\n",
      "762 [D loss: 1.505553] [G loss: -0.616907]\n",
      "763 [D loss: 0.877010] [G loss: -0.668907]\n",
      "764 [D loss: 0.843870] [G loss: -0.739710]\n",
      "765 [D loss: 1.137716] [G loss: -0.787017]\n",
      "766 [D loss: 0.565125] [G loss: -0.757475]\n",
      "767 [D loss: 1.128468] [G loss: -0.718091]\n",
      "768 [D loss: 1.021287] [G loss: -0.658436]\n",
      "769 [D loss: 0.895340] [G loss: -0.632529]\n",
      "770 [D loss: 1.158575] [G loss: -0.812861]\n",
      "771 [D loss: 1.325839] [G loss: -0.847273]\n",
      "772 [D loss: 1.709855] [G loss: -0.950030]\n",
      "773 [D loss: 0.875648] [G loss: -0.961561]\n",
      "774 [D loss: 0.822053] [G loss: -0.925451]\n",
      "775 [D loss: 1.245735] [G loss: -0.934096]\n",
      "776 [D loss: 0.727329] [G loss: -0.979025]\n",
      "777 [D loss: 0.836628] [G loss: -0.978878]\n",
      "778 [D loss: 0.934305] [G loss: -0.981892]\n",
      "779 [D loss: 1.505115] [G loss: -0.989692]\n",
      "780 [D loss: 1.369177] [G loss: -0.991325]\n",
      "781 [D loss: 1.326660] [G loss: -0.943822]\n",
      "782 [D loss: 0.755913] [G loss: -0.875584]\n",
      "783 [D loss: 0.731320] [G loss: -0.660466]\n",
      "784 [D loss: 0.694776] [G loss: -0.697617]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "785 [D loss: 0.847069] [G loss: -0.684955]\n",
      "786 [D loss: 0.716359] [G loss: -0.633295]\n",
      "787 [D loss: 0.637748] [G loss: -0.673228]\n",
      "788 [D loss: 0.802784] [G loss: -0.671513]\n",
      "789 [D loss: 0.554524] [G loss: -0.531609]\n",
      "790 [D loss: 0.503626] [G loss: -0.547973]\n",
      "791 [D loss: 0.649572] [G loss: -0.542519]\n",
      "792 [D loss: 0.850048] [G loss: -0.605624]\n",
      "793 [D loss: 1.424738] [G loss: -0.754253]\n",
      "794 [D loss: 0.861693] [G loss: -0.669887]\n",
      "795 [D loss: 1.278104] [G loss: -0.746841]\n",
      "796 [D loss: 1.245285] [G loss: -0.677835]\n",
      "797 [D loss: 0.820621] [G loss: -0.476619]\n",
      "798 [D loss: 0.409012] [G loss: -0.485580]\n",
      "799 [D loss: 0.615474] [G loss: -0.591911]\n",
      "800 [D loss: 0.603077] [G loss: -0.553826]\n",
      "0.0068434654\n",
      "0.48289102\n",
      "0.016115302\n",
      "0.49081695\n",
      "0.018383486\n",
      "0.4831087\n",
      "0.0285976\n",
      "0.47984943\n",
      "0.015490022\n",
      "0.48509887\n",
      "0.0056814416\n",
      "0.48057967\n",
      "0.01387816\n",
      "0.48920083\n",
      "-0.00874532\n",
      "0.4993934\n",
      "0.0134135075\n",
      "0.49824637\n",
      "0.020628344\n",
      "0.4878209\n",
      "0.01759658\n",
      "0.48412853\n",
      "0.0019188663\n",
      "0.48424217\n",
      "0.013073403\n",
      "0.4960158\n",
      "0.019965533\n",
      "0.48817363\n",
      "0.022109892\n",
      "0.48500162\n",
      "0.02899422\n",
      "0.47622082\n",
      "0.018803127\n",
      "0.47949007\n",
      "0.019613307\n",
      "0.48654765\n",
      "0.026398968\n",
      "0.47181848\n",
      "0.016499355\n",
      "0.4845302\n",
      "0.018433826\n",
      "0.48190323\n",
      "0.028603733\n",
      "0.47598454\n",
      "0.0101727\n",
      "0.48834476\n",
      "0.0010708759\n",
      "0.4881636\n",
      "0.023065848\n",
      "0.48123646\n",
      "801 [D loss: 0.613750] [G loss: -0.534099]\n",
      "802 [D loss: 0.959464] [G loss: -0.634342]\n",
      "803 [D loss: 0.991312] [G loss: -0.675531]\n",
      "804 [D loss: 1.615098] [G loss: -0.735443]\n",
      "805 [D loss: 1.278599] [G loss: -0.740807]\n",
      "806 [D loss: 1.351390] [G loss: -0.557188]\n",
      "807 [D loss: 4.292943] [G loss: -0.693463]\n",
      "808 [D loss: 3.131264] [G loss: -0.836620]\n",
      "809 [D loss: 2.345425] [G loss: -0.917747]\n",
      "810 [D loss: 2.279382] [G loss: -0.942590]\n",
      "811 [D loss: 2.240309] [G loss: -0.940277]\n",
      "812 [D loss: 1.031406] [G loss: -0.956264]\n",
      "813 [D loss: 1.073762] [G loss: -0.938583]\n",
      "814 [D loss: 0.980470] [G loss: -0.946974]\n",
      "815 [D loss: 1.184303] [G loss: -0.931546]\n",
      "816 [D loss: 0.921150] [G loss: -0.865405]\n",
      "817 [D loss: 0.627196] [G loss: -0.818688]\n",
      "818 [D loss: 1.337844] [G loss: -0.898735]\n",
      "819 [D loss: 0.867842] [G loss: -0.960049]\n",
      "820 [D loss: 1.213660] [G loss: -0.966544]\n",
      "821 [D loss: 1.117667] [G loss: -0.976428]\n",
      "822 [D loss: 0.868455] [G loss: -0.979894]\n",
      "823 [D loss: 0.954861] [G loss: -0.983038]\n",
      "824 [D loss: 0.791835] [G loss: -0.977188]\n",
      "825 [D loss: 0.716777] [G loss: -0.973604]\n",
      "826 [D loss: 0.700224] [G loss: -0.967426]\n",
      "827 [D loss: 0.875369] [G loss: -0.977150]\n",
      "828 [D loss: 1.004271] [G loss: -0.975314]\n",
      "829 [D loss: 1.490371] [G loss: -0.944570]\n",
      "830 [D loss: 3.188943] [G loss: -0.978093]\n",
      "831 [D loss: 0.803042] [G loss: -0.986326]\n",
      "832 [D loss: 1.178065] [G loss: -0.990516]\n",
      "833 [D loss: 1.709069] [G loss: -0.988214]\n",
      "834 [D loss: 0.700824] [G loss: -0.988161]\n",
      "835 [D loss: 0.673241] [G loss: -0.989427]\n",
      "836 [D loss: 0.927144] [G loss: -0.987521]\n",
      "837 [D loss: 0.790899] [G loss: -0.989796]\n",
      "838 [D loss: 0.994508] [G loss: -0.987099]\n",
      "839 [D loss: 1.302339] [G loss: -0.985849]\n",
      "840 [D loss: 0.826126] [G loss: -0.978393]\n",
      "841 [D loss: 0.838973] [G loss: -0.986702]\n",
      "842 [D loss: 0.652121] [G loss: -0.993457]\n",
      "843 [D loss: 0.793352] [G loss: -0.994711]\n",
      "844 [D loss: 0.693575] [G loss: -0.994744]\n",
      "845 [D loss: 0.667141] [G loss: -0.992892]\n",
      "846 [D loss: 0.689118] [G loss: -0.994017]\n",
      "847 [D loss: 0.640777] [G loss: -0.990831]\n",
      "848 [D loss: 0.898201] [G loss: -0.987176]\n",
      "849 [D loss: 0.972981] [G loss: -0.945483]\n",
      "850 [D loss: 0.980304] [G loss: -0.888462]\n",
      "851 [D loss: 1.141913] [G loss: -0.608295]\n",
      "852 [D loss: 0.957407] [G loss: -0.646590]\n",
      "853 [D loss: 0.835248] [G loss: -0.549892]\n",
      "854 [D loss: 1.341683] [G loss: -0.313018]\n",
      "855 [D loss: 1.234187] [G loss: -0.543324]\n",
      "856 [D loss: 0.548229] [G loss: -0.531386]\n",
      "857 [D loss: 0.592117] [G loss: -0.555614]\n",
      "858 [D loss: 0.953748] [G loss: -0.531143]\n",
      "859 [D loss: 0.959697] [G loss: -0.422046]\n",
      "860 [D loss: 0.682654] [G loss: -0.582307]\n",
      "861 [D loss: 1.086067] [G loss: -0.488352]\n",
      "862 [D loss: 0.493304] [G loss: -0.452842]\n",
      "863 [D loss: 0.749030] [G loss: -0.541001]\n",
      "864 [D loss: 1.671335] [G loss: -0.560160]\n",
      "865 [D loss: 1.139695] [G loss: -0.659249]\n",
      "866 [D loss: 1.125208] [G loss: -0.728583]\n",
      "867 [D loss: 1.267898] [G loss: -0.785305]\n",
      "868 [D loss: 2.031551] [G loss: -0.896747]\n",
      "869 [D loss: 0.975920] [G loss: -0.926877]\n",
      "870 [D loss: 1.714233] [G loss: -0.949066]\n",
      "871 [D loss: 1.237070] [G loss: -0.967671]\n",
      "872 [D loss: 1.019615] [G loss: -0.959173]\n",
      "873 [D loss: 1.699929] [G loss: -0.971690]\n",
      "874 [D loss: 0.829360] [G loss: -0.952803]\n",
      "875 [D loss: 0.739807] [G loss: -0.927445]\n",
      "876 [D loss: 1.014267] [G loss: -0.957084]\n",
      "877 [D loss: 1.357670] [G loss: -0.980035]\n",
      "878 [D loss: 1.392972] [G loss: -0.983872]\n",
      "879 [D loss: 0.772545] [G loss: -0.983009]\n",
      "880 [D loss: 0.753113] [G loss: -0.986000]\n",
      "881 [D loss: 0.861948] [G loss: -0.983585]\n",
      "882 [D loss: 0.803290] [G loss: -0.983461]\n",
      "883 [D loss: 0.973107] [G loss: -0.967180]\n",
      "884 [D loss: 0.883526] [G loss: -0.931545]\n",
      "885 [D loss: 0.842714] [G loss: -0.739102]\n",
      "886 [D loss: 0.639758] [G loss: -0.526585]\n",
      "887 [D loss: 0.694885] [G loss: -0.664066]\n",
      "888 [D loss: 0.799064] [G loss: -0.446754]\n",
      "889 [D loss: 1.779675] [G loss: -0.783500]\n",
      "890 [D loss: 0.835684] [G loss: -0.764557]\n",
      "891 [D loss: 0.839468] [G loss: -0.833302]\n",
      "892 [D loss: 0.777699] [G loss: -0.857580]\n",
      "893 [D loss: 0.678461] [G loss: -0.775099]\n",
      "894 [D loss: 0.667220] [G loss: -0.871299]\n",
      "895 [D loss: 0.673123] [G loss: -0.867246]\n",
      "896 [D loss: 0.697421] [G loss: -0.796295]\n",
      "897 [D loss: 1.445551] [G loss: -0.633694]\n",
      "898 [D loss: 0.773539] [G loss: -0.549182]\n",
      "899 [D loss: 0.701351] [G loss: -0.507946]\n",
      "900 [D loss: 1.458360] [G loss: -0.624590]\n",
      "901 [D loss: 0.621780] [G loss: -0.601622]\n",
      "902 [D loss: 1.036126] [G loss: -0.701868]\n",
      "903 [D loss: 0.804020] [G loss: -0.727213]\n",
      "904 [D loss: 1.601424] [G loss: -0.603817]\n",
      "905 [D loss: 2.634035] [G loss: -0.602628]\n",
      "906 [D loss: 0.815024] [G loss: -0.574645]\n",
      "907 [D loss: 0.690669] [G loss: -0.634085]\n",
      "908 [D loss: 0.669375] [G loss: -0.673760]\n",
      "909 [D loss: 0.708479] [G loss: -0.658813]\n",
      "910 [D loss: 1.485574] [G loss: -0.630355]\n",
      "911 [D loss: 0.781523] [G loss: -0.609983]\n",
      "912 [D loss: 0.669698] [G loss: -0.704104]\n",
      "913 [D loss: 0.842712] [G loss: -0.545602]\n",
      "914 [D loss: 2.094837] [G loss: -0.695735]\n",
      "915 [D loss: 0.810815] [G loss: -0.589586]\n",
      "916 [D loss: 0.524300] [G loss: -0.441335]\n",
      "917 [D loss: 0.385695] [G loss: -0.532524]\n",
      "918 [D loss: 0.374546] [G loss: -0.415683]\n",
      "919 [D loss: 0.386731] [G loss: -0.556533]\n",
      "920 [D loss: 0.441111] [G loss: -0.410731]\n",
      "921 [D loss: 0.715742] [G loss: -0.592621]\n",
      "922 [D loss: 0.453035] [G loss: -0.620224]\n",
      "923 [D loss: 0.408769] [G loss: -0.706694]\n",
      "924 [D loss: 0.548600] [G loss: -0.730600]\n",
      "925 [D loss: 0.667622] [G loss: -0.741482]\n",
      "926 [D loss: 0.640089] [G loss: -0.789784]\n",
      "927 [D loss: 0.608956] [G loss: -0.771706]\n",
      "928 [D loss: 0.887174] [G loss: -0.727264]\n",
      "929 [D loss: 0.560351] [G loss: -0.534844]\n",
      "930 [D loss: 0.530819] [G loss: -0.626088]\n",
      "931 [D loss: 0.765332] [G loss: -0.722728]\n",
      "932 [D loss: 0.797226] [G loss: -0.601174]\n",
      "933 [D loss: 0.465463] [G loss: -0.502378]\n",
      "934 [D loss: 0.436587] [G loss: -0.622686]\n",
      "935 [D loss: 0.410451] [G loss: -0.501562]\n",
      "936 [D loss: 1.115254] [G loss: -0.348705]\n",
      "937 [D loss: 0.908135] [G loss: -0.648985]\n",
      "938 [D loss: 0.554990] [G loss: -0.786676]\n",
      "939 [D loss: 0.659913] [G loss: -0.803692]\n",
      "940 [D loss: 1.296377] [G loss: -0.739658]\n",
      "941 [D loss: 0.945026] [G loss: -0.755051]\n",
      "942 [D loss: 1.277373] [G loss: -0.865484]\n",
      "943 [D loss: 0.767788] [G loss: -0.832088]\n",
      "944 [D loss: 0.698220] [G loss: -0.738024]\n",
      "945 [D loss: 0.948580] [G loss: -0.738541]\n",
      "946 [D loss: 0.853696] [G loss: -0.811381]\n",
      "947 [D loss: 0.650860] [G loss: -0.828415]\n",
      "948 [D loss: 0.596911] [G loss: -0.721747]\n",
      "949 [D loss: 0.581755] [G loss: -0.721582]\n",
      "950 [D loss: 0.548386] [G loss: -0.735898]\n",
      "951 [D loss: 0.508541] [G loss: -0.753602]\n",
      "952 [D loss: 0.591618] [G loss: -0.772478]\n",
      "953 [D loss: 0.525254] [G loss: -0.726546]\n",
      "954 [D loss: 0.676798] [G loss: -0.456908]\n",
      "955 [D loss: 0.531614] [G loss: -0.614607]\n",
      "956 [D loss: 0.699184] [G loss: -0.456138]\n",
      "957 [D loss: 0.469800] [G loss: -0.343708]\n",
      "958 [D loss: 0.525801] [G loss: -0.466517]\n",
      "959 [D loss: 1.378625] [G loss: -0.499854]\n",
      "960 [D loss: 1.244070] [G loss: -0.625694]\n",
      "961 [D loss: 0.583858] [G loss: -0.617492]\n",
      "962 [D loss: 0.721221] [G loss: -0.679343]\n",
      "963 [D loss: 0.774168] [G loss: -0.646886]\n",
      "964 [D loss: 1.605976] [G loss: -0.787782]\n",
      "965 [D loss: 1.177077] [G loss: -0.721662]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "966 [D loss: 1.281385] [G loss: -0.809467]\n",
      "967 [D loss: 0.591731] [G loss: -0.798502]\n",
      "968 [D loss: 0.610743] [G loss: -0.780452]\n",
      "969 [D loss: 0.606819] [G loss: -0.643180]\n",
      "970 [D loss: 0.496086] [G loss: -0.644551]\n",
      "971 [D loss: 0.488100] [G loss: -0.596161]\n",
      "972 [D loss: 0.410883] [G loss: -0.540190]\n",
      "973 [D loss: 0.769299] [G loss: -0.597626]\n",
      "974 [D loss: 0.513634] [G loss: -0.567567]\n",
      "975 [D loss: 0.510365] [G loss: -0.508470]\n",
      "976 [D loss: 0.680024] [G loss: -0.521145]\n",
      "977 [D loss: 0.805872] [G loss: -0.478327]\n",
      "978 [D loss: 0.898445] [G loss: -0.446693]\n",
      "979 [D loss: 0.602476] [G loss: -0.345865]\n",
      "980 [D loss: 0.334000] [G loss: -0.430823]\n",
      "981 [D loss: 0.559326] [G loss: -0.463153]\n",
      "982 [D loss: 0.644408] [G loss: -0.476599]\n",
      "983 [D loss: 1.578056] [G loss: -0.585043]\n",
      "984 [D loss: 0.721983] [G loss: -0.540549]\n",
      "985 [D loss: 0.880678] [G loss: -0.475516]\n",
      "986 [D loss: 0.937578] [G loss: -0.578979]\n",
      "987 [D loss: 0.453320] [G loss: -0.628639]\n",
      "988 [D loss: 0.526769] [G loss: -0.664469]\n",
      "989 [D loss: 1.194357] [G loss: -0.745944]\n",
      "990 [D loss: 2.129101] [G loss: -0.777207]\n",
      "991 [D loss: 0.970426] [G loss: -0.743308]\n",
      "992 [D loss: 1.188312] [G loss: -0.741982]\n",
      "993 [D loss: 0.939461] [G loss: -0.652869]\n",
      "994 [D loss: 1.016130] [G loss: -0.660276]\n",
      "995 [D loss: 0.657877] [G loss: -0.707739]\n",
      "996 [D loss: 1.024265] [G loss: -0.574890]\n",
      "997 [D loss: 0.891398] [G loss: -0.564380]\n",
      "998 [D loss: 0.802208] [G loss: -0.538152]\n",
      "999 [D loss: 1.679757] [G loss: -0.695425]\n",
      "1000 [D loss: 0.810198] [G loss: -0.638354]\n",
      "1001 [D loss: 0.606992] [G loss: -0.646946]\n",
      "1002 [D loss: 0.601189] [G loss: -0.634039]\n",
      "1003 [D loss: 0.421197] [G loss: -0.664926]\n",
      "1004 [D loss: 0.546153] [G loss: -0.606681]\n",
      "1005 [D loss: 0.526873] [G loss: -0.605868]\n",
      "1006 [D loss: 0.504997] [G loss: -0.564705]\n",
      "1007 [D loss: 0.369111] [G loss: -0.562828]\n",
      "1008 [D loss: 0.392785] [G loss: -0.514282]\n",
      "1009 [D loss: 0.564764] [G loss: -0.587737]\n",
      "1010 [D loss: 0.956259] [G loss: -0.438482]\n",
      "1011 [D loss: 2.056246] [G loss: -0.635196]\n",
      "1012 [D loss: 0.932987] [G loss: -0.509195]\n",
      "1013 [D loss: 0.910338] [G loss: -0.652088]\n",
      "1014 [D loss: 0.422605] [G loss: -0.537520]\n",
      "1015 [D loss: 1.252680] [G loss: -0.529510]\n",
      "1016 [D loss: 0.787698] [G loss: -0.610233]\n",
      "1017 [D loss: 2.015184] [G loss: -0.484927]\n",
      "1018 [D loss: 0.892212] [G loss: -0.426511]\n",
      "1019 [D loss: 0.809825] [G loss: -0.605638]\n",
      "1020 [D loss: 1.129934] [G loss: -0.642213]\n",
      "1021 [D loss: 1.439470] [G loss: -0.670289]\n",
      "1022 [D loss: 0.575153] [G loss: -0.684477]\n",
      "1023 [D loss: 1.042898] [G loss: -0.550613]\n",
      "1024 [D loss: 0.768520] [G loss: -0.626104]\n",
      "1025 [D loss: 2.064148] [G loss: -0.759613]\n",
      "1026 [D loss: 0.684023] [G loss: -0.828697]\n",
      "1027 [D loss: 0.729857] [G loss: -0.880561]\n",
      "1028 [D loss: 1.086378] [G loss: -0.873758]\n",
      "1029 [D loss: 1.093485] [G loss: -0.813274]\n",
      "1030 [D loss: 0.735369] [G loss: -0.872984]\n",
      "1031 [D loss: 0.994429] [G loss: -0.904811]\n",
      "1032 [D loss: 1.456033] [G loss: -0.846708]\n",
      "1033 [D loss: 1.593653] [G loss: -0.805479]\n",
      "1034 [D loss: 0.953765] [G loss: -0.792799]\n",
      "1035 [D loss: 0.936745] [G loss: -0.924138]\n",
      "1036 [D loss: 1.065082] [G loss: -0.907042]\n",
      "1037 [D loss: 1.411379] [G loss: -0.924429]\n",
      "1038 [D loss: 0.940595] [G loss: -0.926932]\n",
      "1039 [D loss: 1.699540] [G loss: -0.959918]\n",
      "1040 [D loss: 1.298134] [G loss: -0.963908]\n",
      "1041 [D loss: 0.942003] [G loss: -0.957409]\n",
      "1042 [D loss: 0.677205] [G loss: -0.918205]\n",
      "1043 [D loss: 1.292869] [G loss: -0.780752]\n",
      "1044 [D loss: 0.913712] [G loss: -0.622560]\n",
      "1045 [D loss: 0.997551] [G loss: -0.727602]\n",
      "1046 [D loss: 0.974764] [G loss: -0.763835]\n",
      "1047 [D loss: 1.178064] [G loss: -0.881828]\n",
      "1048 [D loss: 2.175327] [G loss: -0.844530]\n",
      "1049 [D loss: 0.714274] [G loss: -0.935824]\n",
      "1050 [D loss: 0.792793] [G loss: -0.933227]\n",
      "1051 [D loss: 0.761221] [G loss: -0.913819]\n",
      "1052 [D loss: 0.852989] [G loss: -0.899234]\n",
      "1053 [D loss: 0.757025] [G loss: -0.837545]\n",
      "1054 [D loss: 1.005862] [G loss: -0.903865]\n",
      "1055 [D loss: 1.318128] [G loss: -0.922178]\n",
      "1056 [D loss: 1.151300] [G loss: -0.722998]\n",
      "1057 [D loss: 2.299188] [G loss: -0.564226]\n",
      "1058 [D loss: 1.134429] [G loss: -0.740103]\n",
      "1059 [D loss: 0.887853] [G loss: -0.841082]\n",
      "1060 [D loss: 0.752163] [G loss: -0.850060]\n",
      "1061 [D loss: 0.708360] [G loss: -0.911541]\n",
      "1062 [D loss: 0.709843] [G loss: -0.946871]\n",
      "1063 [D loss: 0.960010] [G loss: -0.949719]\n",
      "1064 [D loss: 0.710982] [G loss: -0.954498]\n",
      "1065 [D loss: 0.726396] [G loss: -0.954764]\n",
      "1066 [D loss: 0.828189] [G loss: -0.952248]\n",
      "1067 [D loss: 0.981606] [G loss: -0.935812]\n",
      "1068 [D loss: 0.754055] [G loss: -0.849116]\n",
      "1069 [D loss: 0.591949] [G loss: -0.857949]\n",
      "1070 [D loss: 0.658176] [G loss: -0.813036]\n",
      "1071 [D loss: 1.289657] [G loss: -0.900575]\n",
      "1072 [D loss: 1.287761] [G loss: -0.886381]\n",
      "1073 [D loss: 1.012733] [G loss: -0.888364]\n",
      "1074 [D loss: 0.607012] [G loss: -0.905702]\n",
      "1075 [D loss: 0.557371] [G loss: -0.920359]\n",
      "1076 [D loss: 0.747254] [G loss: -0.947461]\n",
      "1077 [D loss: 1.366541] [G loss: -0.974681]\n",
      "1078 [D loss: 0.812931] [G loss: -0.989063]\n",
      "1079 [D loss: 0.892450] [G loss: -0.984211]\n",
      "1080 [D loss: 0.835806] [G loss: -0.988418]\n",
      "1081 [D loss: 0.931655] [G loss: -0.988547]\n",
      "1082 [D loss: 1.156790] [G loss: -0.973734]\n",
      "1083 [D loss: 0.725755] [G loss: -0.959447]\n",
      "1084 [D loss: 0.918884] [G loss: -0.898773]\n",
      "1085 [D loss: 0.927286] [G loss: -0.822084]\n",
      "1086 [D loss: 0.792979] [G loss: -0.861323]\n",
      "1087 [D loss: 0.911274] [G loss: -0.883815]\n",
      "1088 [D loss: 0.739219] [G loss: -0.940120]\n",
      "1089 [D loss: 0.758650] [G loss: -0.974839]\n",
      "1090 [D loss: 0.587882] [G loss: -0.986320]\n",
      "1091 [D loss: 0.668344] [G loss: -0.985077]\n",
      "1092 [D loss: 0.675622] [G loss: -0.987493]\n",
      "1093 [D loss: 0.797836] [G loss: -0.984357]\n",
      "1094 [D loss: 1.958829] [G loss: -0.918340]\n",
      "1095 [D loss: 1.655815] [G loss: -0.695109]\n",
      "1096 [D loss: 1.435109] [G loss: -0.452574]\n",
      "1097 [D loss: 0.963522] [G loss: -0.785847]\n",
      "1098 [D loss: 0.955255] [G loss: -0.886007]\n",
      "1099 [D loss: 0.824302] [G loss: -0.940151]\n",
      "1100 [D loss: 0.819189] [G loss: -0.857120]\n",
      "1101 [D loss: 1.416488] [G loss: -0.872009]\n",
      "1102 [D loss: 0.802837] [G loss: -0.952458]\n",
      "1103 [D loss: 1.132265] [G loss: -0.953443]\n",
      "1104 [D loss: 1.014366] [G loss: -0.949663]\n",
      "1105 [D loss: 1.377093] [G loss: -0.979381]\n",
      "1106 [D loss: 1.082749] [G loss: -0.975321]\n",
      "1107 [D loss: 0.946657] [G loss: -0.971694]\n",
      "1108 [D loss: 0.984266] [G loss: -0.937435]\n",
      "1109 [D loss: 1.080350] [G loss: -0.820426]\n",
      "1110 [D loss: 0.826019] [G loss: -0.788162]\n",
      "1111 [D loss: 1.193137] [G loss: -0.862255]\n",
      "1112 [D loss: 1.257733] [G loss: -0.872314]\n",
      "1113 [D loss: 0.781915] [G loss: -0.879486]\n",
      "1114 [D loss: 0.731191] [G loss: -0.919818]\n",
      "1115 [D loss: 0.753337] [G loss: -0.864880]\n",
      "1116 [D loss: 0.757803] [G loss: -0.837518]\n",
      "1117 [D loss: 0.871681] [G loss: -0.787295]\n",
      "1118 [D loss: 1.341588] [G loss: -0.861031]\n",
      "1119 [D loss: 0.759200] [G loss: -0.815596]\n",
      "1120 [D loss: 0.754759] [G loss: -0.912489]\n",
      "1121 [D loss: 0.915659] [G loss: -0.941878]\n",
      "1122 [D loss: 0.956774] [G loss: -0.927311]\n",
      "1123 [D loss: 0.943490] [G loss: -0.952521]\n",
      "1124 [D loss: 0.652704] [G loss: -0.959105]\n",
      "1125 [D loss: 0.728525] [G loss: -0.954567]\n",
      "1126 [D loss: 1.046321] [G loss: -0.959125]\n",
      "1127 [D loss: 1.224664] [G loss: -0.950760]\n",
      "1128 [D loss: 0.811714] [G loss: -0.920698]\n",
      "1129 [D loss: 1.164209] [G loss: -0.881596]\n",
      "1130 [D loss: 0.795392] [G loss: -0.599215]\n",
      "1131 [D loss: 0.660538] [G loss: -0.709059]\n",
      "1132 [D loss: 0.627018] [G loss: -0.774622]\n",
      "1133 [D loss: 0.646403] [G loss: -0.796823]\n",
      "1134 [D loss: 0.548212] [G loss: -0.809764]\n",
      "1135 [D loss: 0.688541] [G loss: -0.797126]\n",
      "1136 [D loss: 0.729220] [G loss: -0.608610]\n",
      "1137 [D loss: 0.479422] [G loss: -0.518630]\n",
      "1138 [D loss: 0.345780] [G loss: -0.375982]\n",
      "1139 [D loss: 0.292196] [G loss: -0.290575]\n",
      "1140 [D loss: 0.154004] [G loss: -0.209361]\n",
      "1141 [D loss: 0.144521] [G loss: -0.196700]\n",
      "1142 [D loss: 0.039498] [G loss: -0.219842]\n",
      "1143 [D loss: 0.165816] [G loss: -0.238019]\n",
      "1144 [D loss: 1.124471] [G loss: -0.182414]\n",
      "1145 [D loss: 0.501866] [G loss: -0.223836]\n",
      "1146 [D loss: 0.284723] [G loss: -0.241179]\n",
      "1147 [D loss: 1.409543] [G loss: -0.416489]\n",
      "1148 [D loss: 1.386038] [G loss: -0.649484]\n",
      "1149 [D loss: 0.523827] [G loss: -0.658878]\n",
      "1150 [D loss: 1.245171] [G loss: -0.831297]\n",
      "1151 [D loss: 0.941422] [G loss: -0.893091]\n",
      "1152 [D loss: 0.611874] [G loss: -0.897567]\n",
      "1153 [D loss: 0.877825] [G loss: -0.895706]\n",
      "1154 [D loss: 0.618260] [G loss: -0.903723]\n",
      "1155 [D loss: 0.695283] [G loss: -0.929772]\n",
      "1156 [D loss: 0.637601] [G loss: -0.898700]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1157 [D loss: 0.725860] [G loss: -0.846746]\n",
      "1158 [D loss: 0.730188] [G loss: -0.770563]\n",
      "1159 [D loss: 0.996091] [G loss: -0.869659]\n",
      "1160 [D loss: 1.107844] [G loss: -0.887264]\n",
      "1161 [D loss: 0.990857] [G loss: -0.850394]\n",
      "1162 [D loss: 1.330496] [G loss: -0.921590]\n",
      "1163 [D loss: 0.691454] [G loss: -0.935794]\n",
      "1164 [D loss: 0.808371] [G loss: -0.941141]\n",
      "1165 [D loss: 0.922451] [G loss: -0.930423]\n",
      "1166 [D loss: 0.970306] [G loss: -0.922095]\n",
      "1167 [D loss: 0.796395] [G loss: -0.885934]\n",
      "1168 [D loss: 0.929182] [G loss: -0.730496]\n",
      "1169 [D loss: 0.764876] [G loss: -0.747577]\n",
      "1170 [D loss: 0.675458] [G loss: -0.763940]\n",
      "1171 [D loss: 0.800066] [G loss: -0.717749]\n",
      "1172 [D loss: 3.175371] [G loss: -0.619247]\n",
      "1173 [D loss: 0.560305] [G loss: -0.464848]\n",
      "1174 [D loss: 0.667496] [G loss: -0.543839]\n",
      "1175 [D loss: 0.767417] [G loss: -0.373725]\n",
      "1176 [D loss: 0.570390] [G loss: -0.359169]\n",
      "1177 [D loss: 0.604621] [G loss: -0.335497]\n",
      "1178 [D loss: 0.852030] [G loss: -0.415596]\n",
      "1179 [D loss: 0.724329] [G loss: -0.471346]\n",
      "1180 [D loss: 1.383297] [G loss: -0.720793]\n",
      "1181 [D loss: 0.540094] [G loss: -0.787534]\n",
      "1182 [D loss: 0.963104] [G loss: -0.927231]\n",
      "1183 [D loss: 1.084223] [G loss: -0.907894]\n",
      "1184 [D loss: 0.942490] [G loss: -0.887075]\n",
      "1185 [D loss: 1.131668] [G loss: -0.887287]\n",
      "1186 [D loss: 1.074405] [G loss: -0.843685]\n",
      "1187 [D loss: 0.575508] [G loss: -0.578591]\n",
      "1188 [D loss: 1.159761] [G loss: -0.741277]\n",
      "1189 [D loss: 0.765626] [G loss: -0.496909]\n",
      "1190 [D loss: 0.871484] [G loss: -0.544743]\n",
      "1191 [D loss: 1.190994] [G loss: -0.709673]\n",
      "1192 [D loss: 0.537174] [G loss: -0.679071]\n",
      "1193 [D loss: 0.537367] [G loss: -0.681434]\n",
      "1194 [D loss: 0.725840] [G loss: -0.618860]\n",
      "1195 [D loss: 0.482910] [G loss: -0.457150]\n",
      "1196 [D loss: 0.762814] [G loss: -0.544929]\n",
      "1197 [D loss: 0.464716] [G loss: -0.419025]\n",
      "1198 [D loss: 0.724715] [G loss: -0.737463]\n",
      "1199 [D loss: 1.298473] [G loss: -0.740360]\n",
      "1200 [D loss: 0.980859] [G loss: -0.788111]\n",
      "0.11581335\n",
      "0.5316527\n",
      "0.1185223\n",
      "0.5241091\n",
      "0.11918937\n",
      "0.5228855\n",
      "0.114221506\n",
      "0.537115\n",
      "0.116676986\n",
      "0.5282071\n",
      "0.11464323\n",
      "0.53613216\n",
      "0.1162781\n",
      "0.5293794\n",
      "0.10996924\n",
      "0.5447243\n",
      "0.11078668\n",
      "0.54475284\n",
      "0.114833094\n",
      "0.53369564\n",
      "0.11995678\n",
      "0.5195038\n",
      "0.11614418\n",
      "0.53162825\n",
      "0.11588286\n",
      "0.52817875\n",
      "0.1197331\n",
      "0.5232184\n",
      "0.12024112\n",
      "0.52147657\n",
      "0.11801244\n",
      "0.5276402\n",
      "0.119544744\n",
      "0.52485263\n",
      "0.120909214\n",
      "0.5176211\n",
      "0.11969284\n",
      "0.5208911\n",
      "0.12126154\n",
      "0.5158164\n",
      "0.118832946\n",
      "0.5226794\n",
      "0.11995341\n",
      "0.5217765\n",
      "0.119775936\n",
      "0.5216178\n",
      "0.11001135\n",
      "0.54981726\n",
      "0.1168149\n",
      "0.52942854\n",
      "1201 [D loss: 0.977448] [G loss: -0.811870]\n",
      "1202 [D loss: 1.412209] [G loss: -0.716845]\n",
      "1203 [D loss: 1.195346] [G loss: -0.661967]\n",
      "1204 [D loss: 0.602716] [G loss: -0.663132]\n",
      "1205 [D loss: 0.704905] [G loss: -0.687407]\n",
      "1206 [D loss: 1.985396] [G loss: -0.639829]\n",
      "1207 [D loss: 0.771063] [G loss: -0.461332]\n",
      "1208 [D loss: 0.495139] [G loss: -0.518146]\n",
      "1209 [D loss: 0.529816] [G loss: -0.627598]\n",
      "1210 [D loss: 0.633686] [G loss: -0.743741]\n",
      "1211 [D loss: 0.675026] [G loss: -0.758432]\n",
      "1212 [D loss: 0.685625] [G loss: -0.697421]\n",
      "1213 [D loss: 0.914586] [G loss: -0.756247]\n",
      "1214 [D loss: 0.692847] [G loss: -0.801359]\n",
      "1215 [D loss: 0.693374] [G loss: -0.828913]\n",
      "1216 [D loss: 0.773492] [G loss: -0.855627]\n",
      "1217 [D loss: 0.759013] [G loss: -0.699351]\n",
      "1218 [D loss: 0.565156] [G loss: -0.746678]\n",
      "1219 [D loss: 0.650735] [G loss: -0.779608]\n",
      "1220 [D loss: 0.709771] [G loss: -0.834769]\n",
      "1221 [D loss: 0.685644] [G loss: -0.918905]\n",
      "1222 [D loss: 0.859370] [G loss: -0.897817]\n",
      "1223 [D loss: 0.767743] [G loss: -0.886210]\n",
      "1224 [D loss: 1.254262] [G loss: -0.779739]\n",
      "1225 [D loss: 0.791830] [G loss: -0.784923]\n",
      "1226 [D loss: 0.745539] [G loss: -0.681879]\n",
      "1227 [D loss: 0.684117] [G loss: -0.361496]\n",
      "1228 [D loss: 1.670107] [G loss: -0.437187]\n",
      "1229 [D loss: 0.513640] [G loss: -0.530089]\n",
      "1230 [D loss: 0.625002] [G loss: -0.478009]\n",
      "1231 [D loss: 0.761722] [G loss: -0.512131]\n",
      "1232 [D loss: 0.565575] [G loss: -0.533823]\n",
      "1233 [D loss: 0.520845] [G loss: -0.550110]\n",
      "1234 [D loss: 0.460939] [G loss: -0.505795]\n",
      "1235 [D loss: 1.333277] [G loss: -0.500538]\n",
      "1236 [D loss: 0.647645] [G loss: -0.441903]\n",
      "1237 [D loss: 1.010143] [G loss: -0.457512]\n",
      "1238 [D loss: 0.390318] [G loss: -0.454390]\n",
      "1239 [D loss: 1.155923] [G loss: -0.516645]\n",
      "1240 [D loss: 0.775182] [G loss: -0.517883]\n",
      "1241 [D loss: 0.639791] [G loss: -0.558968]\n",
      "1242 [D loss: 0.727842] [G loss: -0.564237]\n",
      "1243 [D loss: 0.794347] [G loss: -0.638415]\n",
      "1244 [D loss: 0.850608] [G loss: -0.736889]\n",
      "1245 [D loss: 1.291486] [G loss: -0.751365]\n",
      "1246 [D loss: 0.863945] [G loss: -0.763682]\n",
      "1247 [D loss: 1.142703] [G loss: -0.877950]\n",
      "1248 [D loss: 0.693561] [G loss: -0.890038]\n",
      "1249 [D loss: 0.619818] [G loss: -0.913614]\n",
      "1250 [D loss: 0.801088] [G loss: -0.894671]\n",
      "1251 [D loss: 0.740072] [G loss: -0.861280]\n",
      "1252 [D loss: 0.870291] [G loss: -0.655317]\n",
      "1253 [D loss: 0.621116] [G loss: -0.600508]\n",
      "1254 [D loss: 0.722157] [G loss: -0.540022]\n",
      "1255 [D loss: 0.397206] [G loss: -0.426842]\n",
      "1256 [D loss: 0.450094] [G loss: -0.532131]\n",
      "1257 [D loss: 0.529207] [G loss: -0.403264]\n",
      "1258 [D loss: 0.539099] [G loss: -0.477773]\n",
      "1259 [D loss: 0.565577] [G loss: -0.555814]\n",
      "1260 [D loss: 0.350988] [G loss: -0.548132]\n",
      "1261 [D loss: 0.530992] [G loss: -0.548473]\n",
      "1262 [D loss: 0.393218] [G loss: -0.599092]\n",
      "1263 [D loss: 0.482529] [G loss: -0.652944]\n",
      "1264 [D loss: 0.467225] [G loss: -0.703121]\n",
      "1265 [D loss: 0.534271] [G loss: -0.681964]\n",
      "1266 [D loss: 0.602788] [G loss: -0.688350]\n",
      "1267 [D loss: 0.467182] [G loss: -0.596645]\n",
      "1268 [D loss: 0.454294] [G loss: -0.645496]\n",
      "1269 [D loss: 0.626498] [G loss: -0.647015]\n",
      "1270 [D loss: 1.204518] [G loss: -0.748300]\n",
      "1271 [D loss: 0.495475] [G loss: -0.684988]\n",
      "1272 [D loss: 0.552773] [G loss: -0.716660]\n",
      "1273 [D loss: 0.615201] [G loss: -0.699465]\n",
      "1274 [D loss: 0.670826] [G loss: -0.655343]\n",
      "1275 [D loss: 0.791769] [G loss: -0.666744]\n",
      "1276 [D loss: 0.417847] [G loss: -0.694503]\n",
      "1277 [D loss: 0.485236] [G loss: -0.715022]\n",
      "1278 [D loss: 0.510874] [G loss: -0.712388]\n",
      "1279 [D loss: 0.428212] [G loss: -0.624972]\n",
      "1280 [D loss: 0.542835] [G loss: -0.607657]\n",
      "1281 [D loss: 0.568551] [G loss: -0.447303]\n",
      "1282 [D loss: 0.656250] [G loss: -0.534240]\n",
      "1283 [D loss: 0.351337] [G loss: -0.536639]\n",
      "1284 [D loss: 0.482864] [G loss: -0.550135]\n",
      "1285 [D loss: 0.565171] [G loss: -0.493231]\n",
      "1286 [D loss: 0.729523] [G loss: -0.596580]\n",
      "1287 [D loss: 0.628027] [G loss: -0.654148]\n",
      "1288 [D loss: 0.739263] [G loss: -0.607968]\n",
      "1289 [D loss: 0.905187] [G loss: -0.665273]\n",
      "1290 [D loss: 0.848499] [G loss: -0.567171]\n",
      "1291 [D loss: 0.507186] [G loss: -0.623428]\n",
      "1292 [D loss: 0.795593] [G loss: -0.611562]\n",
      "1293 [D loss: 0.777305] [G loss: -0.532059]\n",
      "1294 [D loss: 1.901627] [G loss: -0.667040]\n",
      "1295 [D loss: 0.795691] [G loss: -0.770283]\n",
      "1296 [D loss: 0.804776] [G loss: -0.706038]\n",
      "1297 [D loss: 0.709044] [G loss: -0.751323]\n",
      "1298 [D loss: 0.753051] [G loss: -0.756552]\n",
      "1299 [D loss: 1.115350] [G loss: -0.775932]\n",
      "1300 [D loss: 0.806611] [G loss: -0.801268]\n",
      "1301 [D loss: 0.660550] [G loss: -0.782842]\n",
      "1302 [D loss: 0.921389] [G loss: -0.747439]\n",
      "1303 [D loss: 0.533594] [G loss: -0.720173]\n",
      "1304 [D loss: 0.515027] [G loss: -0.731549]\n",
      "1305 [D loss: 0.439769] [G loss: -0.733004]\n",
      "1306 [D loss: 0.581478] [G loss: -0.811998]\n",
      "1307 [D loss: 0.666518] [G loss: -0.762441]\n",
      "1308 [D loss: 0.768665] [G loss: -0.830319]\n",
      "1309 [D loss: 0.575400] [G loss: -0.847547]\n",
      "1310 [D loss: 0.781629] [G loss: -0.853587]\n",
      "1311 [D loss: 0.698580] [G loss: -0.868261]\n",
      "1312 [D loss: 0.495497] [G loss: -0.726304]\n",
      "1313 [D loss: 0.598820] [G loss: -0.298622]\n",
      "1314 [D loss: 0.571397] [G loss: -0.345218]\n",
      "1315 [D loss: 1.107580] [G loss: -0.459353]\n",
      "1316 [D loss: 0.495378] [G loss: -0.355057]\n",
      "1317 [D loss: 0.481541] [G loss: -0.481875]\n",
      "1318 [D loss: 0.590693] [G loss: -0.554933]\n",
      "1319 [D loss: 0.697496] [G loss: -0.528163]\n",
      "1320 [D loss: 0.789402] [G loss: -0.546237]\n",
      "1321 [D loss: 2.477859] [G loss: -0.662090]\n",
      "1322 [D loss: 2.001227] [G loss: -0.647748]\n",
      "1323 [D loss: 0.589763] [G loss: -0.680281]\n",
      "1324 [D loss: 0.748400] [G loss: -0.629626]\n",
      "1325 [D loss: 0.794025] [G loss: -0.681651]\n",
      "1326 [D loss: 0.546633] [G loss: -0.644627]\n",
      "1327 [D loss: 0.450739] [G loss: -0.570766]\n",
      "1328 [D loss: 0.586347] [G loss: -0.716471]\n",
      "1329 [D loss: 0.558994] [G loss: -0.746776]\n",
      "1330 [D loss: 0.571100] [G loss: -0.711578]\n",
      "1331 [D loss: 0.478720] [G loss: -0.648148]\n",
      "1332 [D loss: 0.718230] [G loss: -0.646756]\n",
      "1333 [D loss: 0.548472] [G loss: -0.565583]\n",
      "1334 [D loss: 0.782451] [G loss: -0.603644]\n",
      "1335 [D loss: 0.505413] [G loss: -0.619125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1336 [D loss: 0.859907] [G loss: -0.585457]\n",
      "1337 [D loss: 0.712135] [G loss: -0.675638]\n",
      "1338 [D loss: 0.668330] [G loss: -0.745218]\n",
      "1339 [D loss: 0.566681] [G loss: -0.689570]\n",
      "1340 [D loss: 0.442179] [G loss: -0.608047]\n",
      "1341 [D loss: 0.609701] [G loss: -0.739389]\n",
      "1342 [D loss: 0.879601] [G loss: -0.727509]\n",
      "1343 [D loss: 0.612428] [G loss: -0.708701]\n",
      "1344 [D loss: 0.735649] [G loss: -0.859069]\n",
      "1345 [D loss: 0.652610] [G loss: -0.849743]\n",
      "1346 [D loss: 0.669064] [G loss: -0.878089]\n",
      "1347 [D loss: 0.696119] [G loss: -0.799770]\n",
      "1348 [D loss: 0.668869] [G loss: -0.654923]\n",
      "1349 [D loss: 0.594313] [G loss: -0.736607]\n",
      "1350 [D loss: 0.612263] [G loss: -0.749679]\n",
      "1351 [D loss: 0.564744] [G loss: -0.763500]\n",
      "1352 [D loss: 0.720387] [G loss: -0.808649]\n",
      "1353 [D loss: 1.064710] [G loss: -0.731748]\n",
      "1354 [D loss: 1.265840] [G loss: -0.614991]\n",
      "1355 [D loss: 0.907480] [G loss: -0.541452]\n",
      "1356 [D loss: 0.998737] [G loss: -0.700714]\n",
      "1357 [D loss: 1.476114] [G loss: -0.707344]\n",
      "1358 [D loss: 0.934418] [G loss: -0.679828]\n",
      "1359 [D loss: 0.917835] [G loss: -0.581806]\n",
      "1360 [D loss: 1.622337] [G loss: -0.620108]\n",
      "1361 [D loss: 0.575694] [G loss: -0.611669]\n",
      "1362 [D loss: 0.583726] [G loss: -0.699915]\n",
      "1363 [D loss: 0.730872] [G loss: -0.700382]\n",
      "1364 [D loss: 0.392555] [G loss: -0.693596]\n",
      "1365 [D loss: 0.646439] [G loss: -0.733447]\n",
      "1366 [D loss: 0.823099] [G loss: -0.718158]\n",
      "1367 [D loss: 0.443877] [G loss: -0.703318]\n",
      "1368 [D loss: 0.699122] [G loss: -0.666346]\n",
      "1369 [D loss: 0.625338] [G loss: -0.505054]\n",
      "1370 [D loss: 0.716154] [G loss: -0.602350]\n",
      "1371 [D loss: 0.549939] [G loss: -0.728946]\n",
      "1372 [D loss: 0.697608] [G loss: -0.723034]\n",
      "1373 [D loss: 0.449806] [G loss: -0.606119]\n",
      "1374 [D loss: 0.875463] [G loss: -0.617540]\n",
      "1375 [D loss: 0.612784] [G loss: -0.685494]\n",
      "1376 [D loss: 0.985240] [G loss: -0.573636]\n",
      "1377 [D loss: 0.571272] [G loss: -0.570842]\n",
      "1378 [D loss: 1.164904] [G loss: -0.628965]\n",
      "1379 [D loss: 0.926594] [G loss: -0.603149]\n",
      "1380 [D loss: 1.712979] [G loss: -0.688570]\n",
      "1381 [D loss: 0.541737] [G loss: -0.693395]\n",
      "1382 [D loss: 1.051095] [G loss: -0.645617]\n",
      "1383 [D loss: 1.236274] [G loss: -0.706388]\n",
      "1384 [D loss: 0.543759] [G loss: -0.725674]\n",
      "1385 [D loss: 0.549598] [G loss: -0.761419]\n",
      "1386 [D loss: 0.501139] [G loss: -0.756887]\n",
      "1387 [D loss: 0.549547] [G loss: -0.746367]\n",
      "1388 [D loss: 0.611914] [G loss: -0.786110]\n",
      "1389 [D loss: 0.825571] [G loss: -0.796683]\n",
      "1390 [D loss: 0.583220] [G loss: -0.652137]\n",
      "1391 [D loss: 0.594738] [G loss: -0.546384]\n",
      "1392 [D loss: 1.094483] [G loss: -0.665842]\n",
      "1393 [D loss: 0.948651] [G loss: -0.737212]\n",
      "1394 [D loss: 1.046653] [G loss: -0.724648]\n",
      "1395 [D loss: 1.532073] [G loss: -0.703259]\n",
      "1396 [D loss: 0.593802] [G loss: -0.706822]\n",
      "1397 [D loss: 0.673141] [G loss: -0.667417]\n",
      "1398 [D loss: 0.692443] [G loss: -0.600440]\n",
      "1399 [D loss: 0.675840] [G loss: -0.625844]\n",
      "1400 [D loss: 1.375028] [G loss: -0.699340]\n",
      "1401 [D loss: 1.695365] [G loss: -0.712840]\n",
      "1402 [D loss: 0.789393] [G loss: -0.674346]\n",
      "1403 [D loss: 0.446644] [G loss: -0.596458]\n",
      "1404 [D loss: 0.542699] [G loss: -0.702530]\n",
      "1405 [D loss: 1.120000] [G loss: -0.857374]\n",
      "1406 [D loss: 1.961722] [G loss: -0.860980]\n",
      "1407 [D loss: 0.904775] [G loss: -0.841414]\n",
      "1408 [D loss: 1.128584] [G loss: -0.812735]\n",
      "1409 [D loss: 0.754512] [G loss: -0.728199]\n",
      "1410 [D loss: 0.615634] [G loss: -0.811013]\n",
      "1411 [D loss: 0.745116] [G loss: -0.837734]\n",
      "1412 [D loss: 0.694929] [G loss: -0.840357]\n",
      "1413 [D loss: 1.323562] [G loss: -0.857578]\n",
      "1414 [D loss: 0.705147] [G loss: -0.882097]\n",
      "1415 [D loss: 0.659733] [G loss: -0.928123]\n",
      "1416 [D loss: 0.750968] [G loss: -0.917085]\n",
      "1417 [D loss: 0.840391] [G loss: -0.775348]\n",
      "1418 [D loss: 0.804522] [G loss: -0.668560]\n",
      "1419 [D loss: 0.538789] [G loss: -0.599543]\n",
      "1420 [D loss: 0.524505] [G loss: -0.552411]\n",
      "1421 [D loss: 0.827443] [G loss: -0.549924]\n",
      "1422 [D loss: 0.821530] [G loss: -0.477697]\n",
      "1423 [D loss: 2.555283] [G loss: -0.647653]\n",
      "1424 [D loss: 0.634619] [G loss: -0.593539]\n",
      "1425 [D loss: 0.941034] [G loss: -0.667703]\n",
      "1426 [D loss: 0.668441] [G loss: -0.693252]\n",
      "1427 [D loss: 0.812866] [G loss: -0.603393]\n",
      "1428 [D loss: 0.549164] [G loss: -0.578768]\n",
      "1429 [D loss: 0.937836] [G loss: -0.740192]\n",
      "1430 [D loss: 0.993860] [G loss: -0.597645]\n",
      "1431 [D loss: 0.354500] [G loss: -0.584089]\n",
      "1432 [D loss: 0.473260] [G loss: -0.606104]\n",
      "1433 [D loss: 0.374030] [G loss: -0.638774]\n",
      "1434 [D loss: 0.528979] [G loss: -0.601270]\n",
      "1435 [D loss: 0.458953] [G loss: -0.620501]\n",
      "1436 [D loss: 0.515438] [G loss: -0.644665]\n",
      "1437 [D loss: 0.528579] [G loss: -0.660485]\n",
      "1438 [D loss: 0.958302] [G loss: -0.660792]\n",
      "1439 [D loss: 0.884519] [G loss: -0.614713]\n",
      "1440 [D loss: 0.420903] [G loss: -0.594635]\n",
      "1441 [D loss: 0.699298] [G loss: -0.610049]\n",
      "1442 [D loss: 0.736458] [G loss: -0.626356]\n",
      "1443 [D loss: 0.496020] [G loss: -0.646482]\n",
      "1444 [D loss: 1.065524] [G loss: -0.695133]\n",
      "1445 [D loss: 1.777789] [G loss: -0.599740]\n",
      "1446 [D loss: 3.133229] [G loss: -0.594586]\n",
      "1447 [D loss: 1.344663] [G loss: -0.689961]\n",
      "1448 [D loss: 1.191882] [G loss: -0.743712]\n",
      "1449 [D loss: 0.942818] [G loss: -0.700278]\n",
      "1450 [D loss: 1.973833] [G loss: -0.789650]\n",
      "1451 [D loss: 0.761282] [G loss: -0.616861]\n",
      "1452 [D loss: 1.022319] [G loss: -0.752606]\n",
      "1453 [D loss: 1.114929] [G loss: -0.736655]\n",
      "1454 [D loss: 1.587739] [G loss: -0.765147]\n",
      "1455 [D loss: 0.849938] [G loss: -0.810725]\n",
      "1456 [D loss: 0.727929] [G loss: -0.804016]\n",
      "1457 [D loss: 0.718149] [G loss: -0.815870]\n",
      "1458 [D loss: 0.874276] [G loss: -0.736306]\n",
      "1459 [D loss: 0.646071] [G loss: -0.750844]\n",
      "1460 [D loss: 0.566058] [G loss: -0.738294]\n",
      "1461 [D loss: 0.552963] [G loss: -0.743905]\n",
      "1462 [D loss: 0.573419] [G loss: -0.735631]\n",
      "1463 [D loss: 0.621783] [G loss: -0.739006]\n",
      "1464 [D loss: 0.731750] [G loss: -0.714741]\n",
      "1465 [D loss: 0.774420] [G loss: -0.693881]\n",
      "1466 [D loss: 0.590083] [G loss: -0.710019]\n",
      "1467 [D loss: 0.649458] [G loss: -0.575616]\n",
      "1468 [D loss: 0.758993] [G loss: -0.554667]\n",
      "1469 [D loss: 1.967815] [G loss: -0.682082]\n",
      "1470 [D loss: 0.980571] [G loss: -0.633398]\n",
      "1471 [D loss: 0.452685] [G loss: -0.726375]\n",
      "1472 [D loss: 0.451326] [G loss: -0.765784]\n",
      "1473 [D loss: 0.548089] [G loss: -0.739399]\n",
      "1474 [D loss: 0.581537] [G loss: -0.794117]\n",
      "1475 [D loss: 0.636605] [G loss: -0.820508]\n",
      "1476 [D loss: 0.499758] [G loss: -0.820864]\n",
      "1477 [D loss: 0.506583] [G loss: -0.671211]\n",
      "1478 [D loss: 0.495664] [G loss: -0.616178]\n",
      "1479 [D loss: 0.755713] [G loss: -0.780732]\n",
      "1480 [D loss: 0.711754] [G loss: -0.821513]\n",
      "1481 [D loss: 0.812567] [G loss: -0.783358]\n",
      "1482 [D loss: 0.591783] [G loss: -0.756553]\n",
      "1483 [D loss: 0.462119] [G loss: -0.386413]\n",
      "1484 [D loss: 0.466793] [G loss: -0.584993]\n",
      "1485 [D loss: 0.594565] [G loss: -0.749464]\n",
      "1486 [D loss: 0.648360] [G loss: -0.721215]\n",
      "1487 [D loss: 0.952623] [G loss: -0.763076]\n",
      "1488 [D loss: 1.416843] [G loss: -0.730567]\n",
      "1489 [D loss: 1.332548] [G loss: -0.723793]\n",
      "1490 [D loss: 1.338623] [G loss: -0.718115]\n",
      "1491 [D loss: 1.441191] [G loss: -0.554537]\n",
      "1492 [D loss: 0.645976] [G loss: -0.560881]\n",
      "1493 [D loss: 0.559538] [G loss: -0.506650]\n",
      "1494 [D loss: 0.708230] [G loss: -0.434753]\n",
      "1495 [D loss: 1.111573] [G loss: -0.416686]\n",
      "1496 [D loss: 1.692547] [G loss: -0.470403]\n",
      "1497 [D loss: 0.488814] [G loss: -0.439455]\n",
      "1498 [D loss: 0.501416] [G loss: -0.465694]\n",
      "1499 [D loss: 1.108872] [G loss: -0.456260]\n",
      "1500 [D loss: 2.223917] [G loss: -0.487633]\n",
      "1501 [D loss: 0.797571] [G loss: -0.518481]\n",
      "1502 [D loss: 0.453943] [G loss: -0.504295]\n",
      "1503 [D loss: 0.574126] [G loss: -0.639308]\n",
      "1504 [D loss: 0.871081] [G loss: -0.643193]\n",
      "1505 [D loss: 0.938039] [G loss: -0.715913]\n",
      "1506 [D loss: 1.199157] [G loss: -0.752471]\n",
      "1507 [D loss: 0.744707] [G loss: -0.704783]\n",
      "1508 [D loss: 0.792835] [G loss: -0.493332]\n",
      "1509 [D loss: 3.930542] [G loss: -0.614383]\n",
      "1510 [D loss: 0.547535] [G loss: -0.674303]\n",
      "1511 [D loss: 0.492617] [G loss: -0.689628]\n",
      "1512 [D loss: 0.438295] [G loss: -0.701767]\n",
      "1513 [D loss: 0.958657] [G loss: -0.688780]\n",
      "1514 [D loss: 1.135133] [G loss: -0.661078]\n",
      "1515 [D loss: 1.665851] [G loss: -0.643084]\n",
      "1516 [D loss: 0.931622] [G loss: -0.724236]\n",
      "1517 [D loss: 0.855829] [G loss: -0.747573]\n",
      "1518 [D loss: 0.865149] [G loss: -0.758801]\n",
      "1519 [D loss: 0.594810] [G loss: -0.746034]\n",
      "1520 [D loss: 0.773869] [G loss: -0.717250]\n",
      "1521 [D loss: 2.047253] [G loss: -0.766829]\n",
      "1522 [D loss: 0.879606] [G loss: -0.694086]\n",
      "1523 [D loss: 0.558107] [G loss: -0.517849]\n",
      "1524 [D loss: 0.719503] [G loss: -0.636724]\n",
      "1525 [D loss: 0.405931] [G loss: -0.685101]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1526 [D loss: 0.641654] [G loss: -0.711521]\n",
      "1527 [D loss: 0.846063] [G loss: -0.738931]\n",
      "1528 [D loss: 0.628913] [G loss: -0.691436]\n",
      "1529 [D loss: 0.602883] [G loss: -0.710907]\n",
      "1530 [D loss: 0.662357] [G loss: -0.748043]\n",
      "1531 [D loss: 0.659633] [G loss: -0.780853]\n",
      "1532 [D loss: 0.763359] [G loss: -0.785658]\n",
      "1533 [D loss: 0.999019] [G loss: -0.840404]\n",
      "1534 [D loss: 1.946002] [G loss: -0.745916]\n",
      "1535 [D loss: 0.494993] [G loss: -0.673971]\n",
      "1536 [D loss: 0.566512] [G loss: -0.629139]\n",
      "1537 [D loss: 0.788481] [G loss: -0.666059]\n",
      "1538 [D loss: 0.869962] [G loss: -0.590981]\n",
      "1539 [D loss: 0.715090] [G loss: -0.611422]\n",
      "1540 [D loss: 0.566855] [G loss: -0.622309]\n",
      "1541 [D loss: 0.915826] [G loss: -0.489789]\n",
      "1542 [D loss: 0.847563] [G loss: -0.683311]\n",
      "1543 [D loss: 0.627156] [G loss: -0.732020]\n",
      "1544 [D loss: 0.766847] [G loss: -0.706029]\n",
      "1545 [D loss: 0.543356] [G loss: -0.754545]\n",
      "1546 [D loss: 0.652690] [G loss: -0.776230]\n",
      "1547 [D loss: 1.040376] [G loss: -0.754222]\n",
      "1548 [D loss: 1.181058] [G loss: -0.728359]\n",
      "1549 [D loss: 0.519048] [G loss: -0.522047]\n",
      "1550 [D loss: 0.583519] [G loss: -0.592391]\n",
      "1551 [D loss: 0.658614] [G loss: -0.670128]\n",
      "1552 [D loss: 0.680393] [G loss: -0.594348]\n",
      "1553 [D loss: 0.879931] [G loss: -0.591220]\n",
      "1554 [D loss: 1.150080] [G loss: -0.637213]\n",
      "1555 [D loss: 0.699268] [G loss: -0.648486]\n",
      "1556 [D loss: 0.483559] [G loss: -0.656735]\n",
      "1557 [D loss: 0.525773] [G loss: -0.686772]\n",
      "1558 [D loss: 0.718727] [G loss: -0.578653]\n",
      "1559 [D loss: 0.781321] [G loss: -0.731479]\n",
      "1560 [D loss: 0.818744] [G loss: -0.696787]\n",
      "1561 [D loss: 1.064472] [G loss: -0.712363]\n",
      "1562 [D loss: 1.048374] [G loss: -0.630027]\n",
      "1563 [D loss: 0.828689] [G loss: -0.477050]\n",
      "1564 [D loss: 1.338471] [G loss: -0.646692]\n",
      "1565 [D loss: 0.667531] [G loss: -0.575183]\n",
      "1566 [D loss: 1.029165] [G loss: -0.670887]\n",
      "1567 [D loss: 1.162121] [G loss: -0.719377]\n",
      "1568 [D loss: 2.308537] [G loss: -0.805053]\n",
      "1569 [D loss: 0.853096] [G loss: -0.781038]\n",
      "1570 [D loss: 1.868101] [G loss: -0.717369]\n",
      "1571 [D loss: 4.479327] [G loss: -0.821795]\n",
      "1572 [D loss: 1.301641] [G loss: -0.818524]\n",
      "1573 [D loss: 0.837245] [G loss: -0.859513]\n",
      "1574 [D loss: 0.970104] [G loss: -0.859832]\n",
      "1575 [D loss: 0.988701] [G loss: -0.931183]\n",
      "1576 [D loss: 2.063117] [G loss: -0.954515]\n",
      "1577 [D loss: 1.127725] [G loss: -0.960160]\n",
      "1578 [D loss: 0.654563] [G loss: -0.976137]\n",
      "1579 [D loss: 1.247534] [G loss: -0.900140]\n",
      "1580 [D loss: 1.155221] [G loss: -0.889095]\n",
      "1581 [D loss: 0.768320] [G loss: -0.927333]\n",
      "1582 [D loss: 1.467591] [G loss: -0.922664]\n",
      "1583 [D loss: 0.707192] [G loss: -0.898031]\n",
      "1584 [D loss: 0.601580] [G loss: -0.894985]\n",
      "1585 [D loss: 0.931608] [G loss: -0.966196]\n",
      "1586 [D loss: 0.861088] [G loss: -0.984136]\n",
      "1587 [D loss: 0.742695] [G loss: -0.992038]\n",
      "1588 [D loss: 0.765535] [G loss: -0.992448]\n",
      "1589 [D loss: 0.704052] [G loss: -0.995236]\n",
      "1590 [D loss: 0.718899] [G loss: -0.995382]\n",
      "1591 [D loss: 0.874468] [G loss: -0.992833]\n",
      "1592 [D loss: 0.578079] [G loss: -0.995400]\n",
      "1593 [D loss: 0.763295] [G loss: -0.993113]\n",
      "1594 [D loss: 0.845208] [G loss: -0.991947]\n",
      "1595 [D loss: 1.195821] [G loss: -0.995729]\n",
      "1596 [D loss: 0.751181] [G loss: -0.996662]\n",
      "1597 [D loss: 0.727660] [G loss: -0.994366]\n",
      "1598 [D loss: 1.255277] [G loss: -0.995045]\n",
      "1599 [D loss: 0.960935] [G loss: -0.995176]\n",
      "1600 [D loss: 0.865846] [G loss: -0.995709]\n",
      "0.001864471\n",
      "0.57767725\n",
      "0.0033483722\n",
      "0.561431\n",
      "-0.0014397576\n",
      "0.58364373\n",
      "0.0025608712\n",
      "0.57275105\n",
      "-0.002918791\n",
      "0.5737579\n",
      "0.0053454014\n",
      "0.56417274\n",
      "0.0029872223\n",
      "0.57363546\n",
      "0.005562749\n",
      "0.56239694\n",
      "-0.0042532305\n",
      "0.57183576\n",
      "-0.010913247\n",
      "0.5818109\n",
      "-0.0004196487\n",
      "0.5705431\n",
      "0.0029444348\n",
      "0.5758686\n",
      "0.0006092385\n",
      "0.5700683\n",
      "-0.019018319\n",
      "0.5960458\n",
      "0.0013018141\n",
      "0.5735713\n",
      "0.002298152\n",
      "0.5748717\n",
      "-0.0033515906\n",
      "0.5886181\n",
      "0.0016863479\n",
      "0.5773357\n",
      "0.0047386847\n",
      "0.5680391\n",
      "0.0006634053\n",
      "0.5647299\n",
      "0.005028351\n",
      "0.56700206\n",
      "0.003032966\n",
      "0.57415116\n",
      "0.00412736\n",
      "0.5666754\n",
      "0.0019065985\n",
      "0.56956923\n",
      "0.004911555\n",
      "0.56841457\n",
      "1601 [D loss: 0.634141] [G loss: -0.996543]\n",
      "1602 [D loss: 0.678240] [G loss: -0.995604]\n",
      "1603 [D loss: 0.752324] [G loss: -0.988967]\n",
      "1604 [D loss: 0.895613] [G loss: -0.936606]\n",
      "1605 [D loss: 0.700882] [G loss: -0.945196]\n",
      "1606 [D loss: 0.773801] [G loss: -0.947212]\n",
      "1607 [D loss: 0.824173] [G loss: -0.940595]\n",
      "1608 [D loss: 0.785651] [G loss: -0.948600]\n",
      "1609 [D loss: 0.586064] [G loss: -0.951943]\n",
      "1610 [D loss: 0.880226] [G loss: -0.987426]\n",
      "1611 [D loss: 0.978811] [G loss: -0.981712]\n",
      "1612 [D loss: 0.750339] [G loss: -0.952326]\n",
      "1613 [D loss: 0.830202] [G loss: -0.971012]\n",
      "1614 [D loss: 0.568594] [G loss: -0.968101]\n",
      "1615 [D loss: 0.693343] [G loss: -0.966215]\n",
      "1616 [D loss: 1.325943] [G loss: -0.971393]\n",
      "1617 [D loss: 1.133265] [G loss: -0.948687]\n",
      "1618 [D loss: 0.614262] [G loss: -0.857688]\n",
      "1619 [D loss: 0.519737] [G loss: -0.759006]\n",
      "1620 [D loss: 0.851058] [G loss: -0.739557]\n",
      "1621 [D loss: 0.785140] [G loss: -0.809800]\n",
      "1622 [D loss: 1.105266] [G loss: -0.638983]\n",
      "1623 [D loss: 0.730540] [G loss: -0.693727]\n",
      "1624 [D loss: 1.008007] [G loss: -0.763277]\n",
      "1625 [D loss: 2.072507] [G loss: -0.716937]\n",
      "1626 [D loss: 0.853641] [G loss: -0.682179]\n",
      "1627 [D loss: 1.277670] [G loss: -0.695669]\n",
      "1628 [D loss: 0.983015] [G loss: -0.740462]\n",
      "1629 [D loss: 1.143927] [G loss: -0.755509]\n",
      "1630 [D loss: 0.869905] [G loss: -0.783361]\n",
      "1631 [D loss: 0.885343] [G loss: -0.816739]\n",
      "1632 [D loss: 0.905643] [G loss: -0.841668]\n",
      "1633 [D loss: 1.130047] [G loss: -0.794721]\n",
      "1634 [D loss: 1.412064] [G loss: -0.860169]\n",
      "1635 [D loss: 1.190156] [G loss: -0.830554]\n",
      "1636 [D loss: 1.233267] [G loss: -0.843160]\n",
      "1637 [D loss: 2.163205] [G loss: -0.799111]\n",
      "1638 [D loss: 1.309177] [G loss: -0.784982]\n",
      "1639 [D loss: 0.635943] [G loss: -0.645989]\n",
      "1640 [D loss: 0.717835] [G loss: -0.761328]\n",
      "1641 [D loss: 1.281639] [G loss: -0.669720]\n",
      "1642 [D loss: 0.802968] [G loss: -0.636524]\n",
      "1643 [D loss: 0.678070] [G loss: -0.731021]\n",
      "1644 [D loss: 0.553034] [G loss: -0.761857]\n",
      "1645 [D loss: 0.634892] [G loss: -0.588129]\n",
      "1646 [D loss: 0.577385] [G loss: -0.325055]\n",
      "1647 [D loss: 0.283256] [G loss: -0.380458]\n",
      "1648 [D loss: 0.445778] [G loss: -0.436980]\n",
      "1649 [D loss: 0.389943] [G loss: -0.457071]\n",
      "1650 [D loss: 0.584615] [G loss: -0.521600]\n",
      "1651 [D loss: 0.515841] [G loss: -0.596078]\n",
      "1652 [D loss: 0.778012] [G loss: -0.606502]\n",
      "1653 [D loss: 0.818185] [G loss: -0.594652]\n",
      "1654 [D loss: 0.835699] [G loss: -0.595647]\n",
      "1655 [D loss: 1.644154] [G loss: -0.644028]\n",
      "1656 [D loss: 0.574011] [G loss: -0.607739]\n",
      "1657 [D loss: 0.353358] [G loss: -0.570544]\n",
      "1658 [D loss: 0.918247] [G loss: -0.644154]\n",
      "1659 [D loss: 0.829085] [G loss: -0.682167]\n",
      "1660 [D loss: 1.204923] [G loss: -0.719124]\n",
      "1661 [D loss: 2.845320] [G loss: -0.846795]\n",
      "1662 [D loss: 0.870213] [G loss: -0.899475]\n",
      "1663 [D loss: 0.841426] [G loss: -0.879418]\n",
      "1664 [D loss: 2.210762] [G loss: -0.866788]\n",
      "1665 [D loss: 1.443748] [G loss: -0.864278]\n",
      "1666 [D loss: 0.586674] [G loss: -0.834390]\n",
      "1667 [D loss: 0.920463] [G loss: -0.878422]\n",
      "1668 [D loss: 0.769332] [G loss: -0.873226]\n",
      "1669 [D loss: 1.077496] [G loss: -0.797641]\n",
      "1670 [D loss: 1.211426] [G loss: -0.838296]\n",
      "1671 [D loss: 0.915969] [G loss: -0.850840]\n",
      "1672 [D loss: 1.665723] [G loss: -0.882477]\n",
      "1673 [D loss: 0.763242] [G loss: -0.836653]\n",
      "1674 [D loss: 0.672153] [G loss: -0.764248]\n",
      "1675 [D loss: 0.714078] [G loss: -0.746149]\n",
      "1676 [D loss: 0.838002] [G loss: -0.808838]\n",
      "1677 [D loss: 1.419201] [G loss: -0.760817]\n",
      "1678 [D loss: 0.905213] [G loss: -0.717954]\n",
      "1679 [D loss: 0.545709] [G loss: -0.693344]\n",
      "1680 [D loss: 0.487912] [G loss: -0.773117]\n",
      "1681 [D loss: 0.751724] [G loss: -0.720557]\n",
      "1682 [D loss: 0.979591] [G loss: -0.551081]\n",
      "1683 [D loss: 2.396535] [G loss: -0.620738]\n",
      "1684 [D loss: 0.686295] [G loss: -0.676542]\n",
      "1685 [D loss: 0.683009] [G loss: -0.605482]\n",
      "1686 [D loss: 0.439646] [G loss: -0.688174]\n",
      "1687 [D loss: 0.636549] [G loss: -0.659744]\n",
      "1688 [D loss: 0.443138] [G loss: -0.596840]\n",
      "1689 [D loss: 0.415201] [G loss: -0.631978]\n",
      "1690 [D loss: 0.417090] [G loss: -0.679128]\n",
      "1691 [D loss: 0.629911] [G loss: -0.769072]\n",
      "1692 [D loss: 0.787448] [G loss: -0.787030]\n",
      "1693 [D loss: 0.588880] [G loss: -0.805943]\n",
      "1694 [D loss: 0.687603] [G loss: -0.790796]\n",
      "1695 [D loss: 0.594497] [G loss: -0.761067]\n",
      "1696 [D loss: 0.722135] [G loss: -0.692300]\n",
      "1697 [D loss: 0.803500] [G loss: -0.609333]\n",
      "1698 [D loss: 0.372905] [G loss: -0.582573]\n",
      "1699 [D loss: 0.633689] [G loss: -0.679079]\n",
      "1700 [D loss: 0.428567] [G loss: -0.602478]\n",
      "1701 [D loss: 0.417092] [G loss: -0.640874]\n",
      "1702 [D loss: 0.515349] [G loss: -0.648649]\n",
      "1703 [D loss: 0.512312] [G loss: -0.644278]\n",
      "1704 [D loss: 0.579280] [G loss: -0.593991]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1705 [D loss: 0.839057] [G loss: -0.673434]\n",
      "1706 [D loss: 0.614669] [G loss: -0.661092]\n",
      "1707 [D loss: 0.431183] [G loss: -0.637454]\n",
      "1708 [D loss: 0.615295] [G loss: -0.626537]\n",
      "1709 [D loss: 0.473512] [G loss: -0.622910]\n",
      "1710 [D loss: 0.800019] [G loss: -0.660471]\n",
      "1711 [D loss: 0.641649] [G loss: -0.681932]\n",
      "1712 [D loss: 0.579926] [G loss: -0.668680]\n",
      "1713 [D loss: 0.378958] [G loss: -0.654683]\n",
      "1714 [D loss: 0.924342] [G loss: -0.638514]\n",
      "1715 [D loss: 0.985626] [G loss: -0.639807]\n",
      "1716 [D loss: 0.462019] [G loss: -0.667127]\n",
      "1717 [D loss: 0.607179] [G loss: -0.714488]\n",
      "1718 [D loss: 0.811309] [G loss: -0.734159]\n",
      "1719 [D loss: 1.060717] [G loss: -0.720353]\n",
      "1720 [D loss: 0.799404] [G loss: -0.665596]\n",
      "1721 [D loss: 0.471200] [G loss: -0.739473]\n",
      "1722 [D loss: 0.457013] [G loss: -0.695369]\n",
      "1723 [D loss: 0.521480] [G loss: -0.707956]\n",
      "1724 [D loss: 0.508446] [G loss: -0.717634]\n",
      "1725 [D loss: 0.813510] [G loss: -0.726648]\n",
      "1726 [D loss: 1.281873] [G loss: -0.775992]\n",
      "1727 [D loss: 0.629911] [G loss: -0.764576]\n",
      "1728 [D loss: 0.599270] [G loss: -0.725452]\n",
      "1729 [D loss: 0.764978] [G loss: -0.780483]\n",
      "1730 [D loss: 0.916870] [G loss: -0.778748]\n",
      "1731 [D loss: 0.679957] [G loss: -0.682830]\n",
      "1732 [D loss: 0.853970] [G loss: -0.587878]\n",
      "1733 [D loss: 0.879128] [G loss: -0.689104]\n",
      "1734 [D loss: 1.113720] [G loss: -0.741975]\n",
      "1735 [D loss: 1.069103] [G loss: -0.762784]\n",
      "1736 [D loss: 0.894206] [G loss: -0.765360]\n",
      "1737 [D loss: 0.798807] [G loss: -0.665405]\n",
      "1738 [D loss: 0.786253] [G loss: -0.691541]\n",
      "1739 [D loss: 0.512310] [G loss: -0.757098]\n",
      "1740 [D loss: 0.481927] [G loss: -0.746557]\n",
      "1741 [D loss: 0.767232] [G loss: -0.682801]\n",
      "1742 [D loss: 0.564812] [G loss: -0.687129]\n",
      "1743 [D loss: 0.432282] [G loss: -0.749956]\n",
      "1744 [D loss: 0.595946] [G loss: -0.746876]\n",
      "1745 [D loss: 0.756355] [G loss: -0.770439]\n",
      "1746 [D loss: 0.730629] [G loss: -0.801895]\n",
      "1747 [D loss: 0.709943] [G loss: -0.822972]\n",
      "1748 [D loss: 0.875613] [G loss: -0.808713]\n",
      "1749 [D loss: 2.330858] [G loss: -0.765445]\n",
      "1750 [D loss: 0.693698] [G loss: -0.841996]\n",
      "1751 [D loss: 0.601829] [G loss: -0.851599]\n",
      "1752 [D loss: 0.580013] [G loss: -0.861467]\n",
      "1753 [D loss: 0.600530] [G loss: -0.812136]\n",
      "1754 [D loss: 0.771600] [G loss: -0.805264]\n",
      "1755 [D loss: 0.758485] [G loss: -0.848753]\n",
      "1756 [D loss: 0.866960] [G loss: -0.841629]\n",
      "1757 [D loss: 2.182129] [G loss: -0.860200]\n",
      "1758 [D loss: 0.541198] [G loss: -0.860092]\n",
      "1759 [D loss: 0.807944] [G loss: -0.828504]\n",
      "1760 [D loss: 0.622171] [G loss: -0.775893]\n",
      "1761 [D loss: 0.663783] [G loss: -0.718822]\n",
      "1762 [D loss: 0.608049] [G loss: -0.721635]\n",
      "1763 [D loss: 0.529502] [G loss: -0.807512]\n",
      "1764 [D loss: 0.725929] [G loss: -0.784882]\n",
      "1765 [D loss: 0.909270] [G loss: -0.737349]\n",
      "1766 [D loss: 1.590071] [G loss: -0.835982]\n",
      "1767 [D loss: 0.745016] [G loss: -0.874173]\n",
      "1768 [D loss: 0.695220] [G loss: -0.882403]\n",
      "1769 [D loss: 1.156165] [G loss: -0.934461]\n",
      "1770 [D loss: 1.939425] [G loss: -0.907597]\n",
      "1771 [D loss: 0.842338] [G loss: -0.939306]\n",
      "1772 [D loss: 1.075085] [G loss: -0.967627]\n",
      "1773 [D loss: 1.137339] [G loss: -0.944903]\n",
      "1774 [D loss: 1.101998] [G loss: -0.870818]\n",
      "1775 [D loss: 0.906796] [G loss: -0.937720]\n",
      "1776 [D loss: 0.843689] [G loss: -0.951221]\n",
      "1777 [D loss: 0.840680] [G loss: -0.969750]\n",
      "1778 [D loss: 0.912473] [G loss: -0.973353]\n",
      "1779 [D loss: 0.604833] [G loss: -0.978061]\n",
      "1780 [D loss: 0.706966] [G loss: -0.984858]\n",
      "1781 [D loss: 0.980775] [G loss: -0.989856]\n",
      "1782 [D loss: 0.754430] [G loss: -0.992686]\n",
      "1783 [D loss: 0.611653] [G loss: -0.992862]\n",
      "1784 [D loss: 0.804010] [G loss: -0.992973]\n",
      "1785 [D loss: 0.657007] [G loss: -0.985422]\n",
      "1786 [D loss: 0.563657] [G loss: -0.994415]\n",
      "1787 [D loss: 0.737519] [G loss: -0.994788]\n",
      "1788 [D loss: 0.967076] [G loss: -0.995552]\n",
      "1789 [D loss: 0.978923] [G loss: -0.996289]\n",
      "1790 [D loss: 0.888450] [G loss: -0.997486]\n",
      "1791 [D loss: 0.852325] [G loss: -0.998014]\n",
      "1792 [D loss: 0.842562] [G loss: -0.997663]\n",
      "1793 [D loss: 0.883820] [G loss: -0.997458]\n",
      "1794 [D loss: 0.733136] [G loss: -0.998073]\n",
      "1795 [D loss: 0.541147] [G loss: -0.996942]\n",
      "1796 [D loss: 0.511864] [G loss: -0.992069]\n",
      "1797 [D loss: 0.518477] [G loss: -0.956786]\n",
      "1798 [D loss: 0.607248] [G loss: -0.987139]\n",
      "1799 [D loss: 0.562092] [G loss: -0.995230]\n",
      "1800 [D loss: 0.678800] [G loss: -0.997309]\n",
      "1801 [D loss: 0.840078] [G loss: -0.997089]\n",
      "1802 [D loss: 1.010649] [G loss: -0.998557]\n",
      "1803 [D loss: 0.617843] [G loss: -0.999094]\n",
      "1804 [D loss: 0.525731] [G loss: -0.999125]\n",
      "1805 [D loss: 0.614057] [G loss: -0.999321]\n",
      "1806 [D loss: 0.838607] [G loss: -0.999087]\n",
      "1807 [D loss: 0.954931] [G loss: -0.999676]\n",
      "1808 [D loss: 1.065319] [G loss: -0.999631]\n",
      "1809 [D loss: 0.483928] [G loss: -0.999732]\n",
      "1810 [D loss: 0.683949] [G loss: -0.999714]\n",
      "1811 [D loss: 0.847539] [G loss: -0.999767]\n",
      "1812 [D loss: 0.639464] [G loss: -0.999746]\n",
      "1813 [D loss: 0.391437] [G loss: -0.999808]\n",
      "1814 [D loss: 0.545216] [G loss: -0.999874]\n",
      "1815 [D loss: 0.576605] [G loss: -0.999877]\n",
      "1816 [D loss: 0.630802] [G loss: -0.999823]\n",
      "1817 [D loss: 0.642948] [G loss: -0.999828]\n",
      "1818 [D loss: 0.329963] [G loss: -0.999832]\n",
      "1819 [D loss: 0.435796] [G loss: -0.999791]\n",
      "1820 [D loss: 0.580267] [G loss: -0.999860]\n",
      "1821 [D loss: 0.511929] [G loss: -0.999832]\n",
      "1822 [D loss: 0.482311] [G loss: -0.999897]\n",
      "1823 [D loss: 0.452131] [G loss: -0.999782]\n",
      "1824 [D loss: 0.835966] [G loss: -0.999747]\n",
      "1825 [D loss: 1.402413] [G loss: -0.999756]\n",
      "1826 [D loss: 0.486479] [G loss: -0.999845]\n",
      "1827 [D loss: 0.428559] [G loss: -0.999900]\n",
      "1828 [D loss: 0.479040] [G loss: -0.999940]\n",
      "1829 [D loss: 0.660701] [G loss: -0.999946]\n",
      "1830 [D loss: 0.408418] [G loss: -0.999931]\n",
      "1831 [D loss: 0.446532] [G loss: -0.999946]\n",
      "1832 [D loss: 0.444358] [G loss: -0.999942]\n",
      "1833 [D loss: 1.345530] [G loss: -0.999909]\n",
      "1834 [D loss: 0.625641] [G loss: -0.999939]\n",
      "1835 [D loss: 0.381070] [G loss: -0.999933]\n",
      "1836 [D loss: 0.402714] [G loss: -0.999950]\n",
      "1837 [D loss: 0.323932] [G loss: -0.999966]\n",
      "1838 [D loss: 0.403513] [G loss: -0.999941]\n",
      "1839 [D loss: 0.545108] [G loss: -0.999936]\n",
      "1840 [D loss: 0.606486] [G loss: -0.999953]\n",
      "1841 [D loss: 0.554976] [G loss: -0.999976]\n",
      "1842 [D loss: 0.364791] [G loss: -0.999979]\n",
      "1843 [D loss: 0.420116] [G loss: -0.999978]\n",
      "1844 [D loss: 0.381995] [G loss: -0.999977]\n",
      "1845 [D loss: 0.323948] [G loss: -0.999982]\n",
      "1846 [D loss: 0.447912] [G loss: -0.999989]\n",
      "1847 [D loss: 0.474059] [G loss: -0.999987]\n",
      "1848 [D loss: 0.496364] [G loss: -0.999975]\n",
      "1849 [D loss: 0.418105] [G loss: -0.999981]\n",
      "1850 [D loss: 0.610261] [G loss: -0.999978]\n",
      "1851 [D loss: 0.393149] [G loss: -0.999985]\n",
      "1852 [D loss: 0.413534] [G loss: -0.999988]\n",
      "1853 [D loss: 0.554946] [G loss: -0.999983]\n",
      "1854 [D loss: 0.551177] [G loss: -0.999975]\n",
      "1855 [D loss: 0.400981] [G loss: -0.999974]\n",
      "1856 [D loss: 0.340963] [G loss: -0.999985]\n",
      "1857 [D loss: 0.405838] [G loss: -0.999986]\n",
      "1858 [D loss: 0.522157] [G loss: -0.999976]\n",
      "1859 [D loss: 0.391867] [G loss: -0.999988]\n",
      "1860 [D loss: 0.257381] [G loss: -0.999991]\n",
      "1861 [D loss: 0.419068] [G loss: -0.999983]\n",
      "1862 [D loss: 0.228032] [G loss: -0.999990]\n",
      "1863 [D loss: 0.236843] [G loss: -0.999987]\n",
      "1864 [D loss: 0.315494] [G loss: -0.999988]\n",
      "1865 [D loss: 0.376729] [G loss: -0.999989]\n",
      "1866 [D loss: 0.309080] [G loss: -0.999991]\n",
      "1867 [D loss: 0.469527] [G loss: -0.999972]\n",
      "1868 [D loss: 0.306635] [G loss: -0.999991]\n",
      "1869 [D loss: 0.244891] [G loss: -0.999994]\n",
      "1870 [D loss: 0.385381] [G loss: -0.999992]\n",
      "1871 [D loss: 0.256462] [G loss: -0.999993]\n",
      "1872 [D loss: 0.308754] [G loss: -0.999995]\n",
      "1873 [D loss: 0.201774] [G loss: -0.999994]\n",
      "1874 [D loss: 0.208084] [G loss: -0.999992]\n",
      "1875 [D loss: 0.209152] [G loss: -0.999994]\n",
      "1876 [D loss: 0.329748] [G loss: -0.999993]\n",
      "1877 [D loss: 0.401392] [G loss: -0.999994]\n",
      "1878 [D loss: 0.461291] [G loss: -0.999996]\n",
      "1879 [D loss: 0.315756] [G loss: -0.999995]\n",
      "1880 [D loss: 0.281944] [G loss: -0.999996]\n",
      "1881 [D loss: 0.256092] [G loss: -0.999996]\n",
      "1882 [D loss: 0.278798] [G loss: -0.999995]\n",
      "1883 [D loss: 0.373764] [G loss: -0.999995]\n",
      "1884 [D loss: 0.277965] [G loss: -0.999996]\n",
      "1885 [D loss: 0.170301] [G loss: -0.999995]\n",
      "1886 [D loss: 0.419129] [G loss: -0.999990]\n",
      "1887 [D loss: 0.360289] [G loss: -0.999975]\n",
      "1888 [D loss: 0.247880] [G loss: -0.999850]\n",
      "1889 [D loss: 0.242330] [G loss: -0.995479]\n",
      "1890 [D loss: 0.386235] [G loss: -0.999125]\n",
      "1891 [D loss: 0.465019] [G loss: -0.999844]\n",
      "1892 [D loss: 0.293540] [G loss: -0.999897]\n",
      "1893 [D loss: 0.265341] [G loss: -0.999931]\n",
      "1894 [D loss: 0.430083] [G loss: -0.999902]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1895 [D loss: 0.227027] [G loss: -0.999945]\n",
      "1896 [D loss: 0.217247] [G loss: -0.999950]\n",
      "1897 [D loss: 0.200078] [G loss: -0.999960]\n",
      "1898 [D loss: 0.401315] [G loss: -0.999964]\n",
      "1899 [D loss: 0.597748] [G loss: -0.999961]\n",
      "1900 [D loss: 0.513677] [G loss: -0.999969]\n",
      "1901 [D loss: 2.009772] [G loss: -0.999948]\n",
      "1902 [D loss: 0.300835] [G loss: -0.999974]\n",
      "1903 [D loss: 0.333298] [G loss: -0.999980]\n",
      "1904 [D loss: 0.429865] [G loss: -0.999974]\n",
      "1905 [D loss: 0.381431] [G loss: -0.999985]\n",
      "1906 [D loss: 0.199226] [G loss: -0.999985]\n",
      "1907 [D loss: 0.324262] [G loss: -0.999984]\n",
      "1908 [D loss: 0.223379] [G loss: -0.999984]\n",
      "1909 [D loss: 0.853962] [G loss: -0.999988]\n",
      "1910 [D loss: 0.371764] [G loss: -0.999989]\n",
      "1911 [D loss: 0.252518] [G loss: -0.999990]\n",
      "1912 [D loss: 0.188137] [G loss: -0.999991]\n",
      "1913 [D loss: 0.178056] [G loss: -0.999991]\n",
      "1914 [D loss: 0.392832] [G loss: -0.999993]\n",
      "1915 [D loss: 0.209806] [G loss: -0.999992]\n",
      "1916 [D loss: 0.332447] [G loss: -0.999993]\n",
      "1917 [D loss: 0.178686] [G loss: -0.999993]\n",
      "1918 [D loss: 0.271071] [G loss: -0.999994]\n",
      "1919 [D loss: 0.367462] [G loss: -0.999989]\n",
      "1920 [D loss: 0.407982] [G loss: -0.999994]\n",
      "1921 [D loss: 0.145340] [G loss: -0.999995]\n",
      "1922 [D loss: 0.147549] [G loss: -0.999993]\n",
      "1923 [D loss: 0.467735] [G loss: -0.999996]\n",
      "1924 [D loss: 0.171735] [G loss: -0.999996]\n",
      "1925 [D loss: 0.373543] [G loss: -0.999996]\n",
      "1926 [D loss: 0.445801] [G loss: -0.999997]\n",
      "1927 [D loss: 0.374787] [G loss: -0.999994]\n",
      "1928 [D loss: 0.400651] [G loss: -0.999997]\n",
      "1929 [D loss: 0.537883] [G loss: -0.999998]\n",
      "1930 [D loss: 0.599662] [G loss: -0.999997]\n",
      "1931 [D loss: 0.210464] [G loss: -0.999997]\n",
      "1932 [D loss: 0.326237] [G loss: -0.999998]\n",
      "1933 [D loss: 0.195587] [G loss: -0.999998]\n",
      "1934 [D loss: 0.438722] [G loss: -0.999998]\n",
      "1935 [D loss: 0.401259] [G loss: -0.999998]\n",
      "1936 [D loss: 0.270049] [G loss: -0.999998]\n",
      "1937 [D loss: 0.202047] [G loss: -0.999999]\n",
      "1938 [D loss: 0.205605] [G loss: -0.999999]\n",
      "1939 [D loss: 0.173407] [G loss: -0.999998]\n",
      "1940 [D loss: 0.287023] [G loss: -0.999998]\n",
      "1941 [D loss: 0.287292] [G loss: -0.999999]\n",
      "1942 [D loss: 0.272871] [G loss: -0.999998]\n",
      "1943 [D loss: 0.358561] [G loss: -0.999999]\n",
      "1944 [D loss: 0.225985] [G loss: -0.999999]\n",
      "1945 [D loss: 0.255382] [G loss: -0.999999]\n",
      "1946 [D loss: 0.377479] [G loss: -0.999999]\n",
      "1947 [D loss: 0.496537] [G loss: -0.999999]\n",
      "1948 [D loss: 1.074027] [G loss: -0.999998]\n",
      "1949 [D loss: 2.055306] [G loss: -0.999997]\n",
      "1950 [D loss: 0.234342] [G loss: -0.999999]\n",
      "1951 [D loss: 0.263694] [G loss: -0.999999]\n",
      "1952 [D loss: 0.187472] [G loss: -0.999999]\n",
      "1953 [D loss: 0.180776] [G loss: -0.999999]\n",
      "1954 [D loss: 0.355597] [G loss: -0.999999]\n",
      "1955 [D loss: 0.253558] [G loss: -0.999999]\n",
      "1956 [D loss: 0.198820] [G loss: -0.999999]\n",
      "1957 [D loss: 0.284789] [G loss: -0.999999]\n",
      "1958 [D loss: 0.173508] [G loss: -0.999999]\n",
      "1959 [D loss: 0.167441] [G loss: -0.999999]\n",
      "1960 [D loss: 0.214781] [G loss: -0.999999]\n",
      "1961 [D loss: 0.170720] [G loss: -1.000000]\n",
      "1962 [D loss: 0.214211] [G loss: -0.999999]\n",
      "1963 [D loss: 0.153966] [G loss: -0.999999]\n",
      "1964 [D loss: 0.163609] [G loss: -0.999999]\n",
      "1965 [D loss: 0.187966] [G loss: -1.000000]\n",
      "1966 [D loss: 0.143904] [G loss: -1.000000]\n",
      "1967 [D loss: 0.181849] [G loss: -1.000000]\n",
      "1968 [D loss: 0.312709] [G loss: -1.000000]\n",
      "1969 [D loss: 0.192409] [G loss: -1.000000]\n",
      "1970 [D loss: 0.229817] [G loss: -1.000000]\n",
      "1971 [D loss: 0.199297] [G loss: -1.000000]\n",
      "1972 [D loss: 0.235935] [G loss: -1.000000]\n",
      "1973 [D loss: 0.193685] [G loss: -1.000000]\n",
      "1974 [D loss: 0.196696] [G loss: -1.000000]\n",
      "1975 [D loss: 0.261615] [G loss: -1.000000]\n",
      "1976 [D loss: 0.280770] [G loss: -1.000000]\n",
      "1977 [D loss: 0.199251] [G loss: -1.000000]\n",
      "1978 [D loss: 0.203312] [G loss: -1.000000]\n",
      "1979 [D loss: 0.226344] [G loss: -1.000000]\n",
      "1980 [D loss: 0.339960] [G loss: -1.000000]\n",
      "1981 [D loss: 0.108680] [G loss: -1.000000]\n",
      "1982 [D loss: 0.137355] [G loss: -1.000000]\n",
      "1983 [D loss: 0.307901] [G loss: -1.000000]\n",
      "1984 [D loss: 0.312366] [G loss: -1.000000]\n",
      "1985 [D loss: 0.804348] [G loss: -1.000000]\n",
      "1986 [D loss: 0.111728] [G loss: -1.000000]\n",
      "1987 [D loss: 0.161201] [G loss: -1.000000]\n",
      "1988 [D loss: 0.186541] [G loss: -1.000000]\n",
      "1989 [D loss: 0.793131] [G loss: -1.000000]\n",
      "1990 [D loss: 0.573994] [G loss: -1.000000]\n",
      "1991 [D loss: 0.139672] [G loss: -1.000000]\n",
      "1992 [D loss: 0.189424] [G loss: -1.000000]\n",
      "1993 [D loss: 0.278640] [G loss: -1.000000]\n",
      "1994 [D loss: 0.435218] [G loss: -1.000000]\n",
      "1995 [D loss: 0.271445] [G loss: -1.000000]\n",
      "1996 [D loss: 0.321966] [G loss: -1.000000]\n",
      "1997 [D loss: 0.177198] [G loss: -1.000000]\n",
      "1998 [D loss: 0.308917] [G loss: -1.000000]\n",
      "1999 [D loss: 0.117951] [G loss: -1.000000]\n"
     ]
    }
   ],
   "source": [
    "from keras.layers.merge import _Merge\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU, ReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D, Conv2DTranspose\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam, RMSprop\n",
    "from functools import partial\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class RandomWeightedAverage(_Merge):\n",
    "    \"\"\"Provides a (random) weighted average between real and generated image samples\"\"\"\n",
    "    def _merge_function(self, inputs):\n",
    "        alpha = K.random_uniform((32, 1, 1, 1))\n",
    "        return (alpha * inputs[0]) + ((1 - alpha) * inputs[1])\n",
    "\n",
    "class GCBD():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 64\n",
    "        self.img_cols = 64\n",
    "        self.channels = 1\n",
    "        self.img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "        self.latent_dim = 128\n",
    "\n",
    "        optimizer = RMSprop()\n",
    "\n",
    "        # Build the generator and discriminator\n",
    "        self.generator = self.build_generator()\n",
    "        self.discriminator = self.build_discriminator()\n",
    "\n",
    "        # Freeze generator's layers while training discriminator\n",
    "        self.generator.trainable = False\n",
    "\n",
    "        # Image input (real sample)\n",
    "        real_img = Input(shape=self.img_shape)\n",
    "\n",
    "        # Noise input\n",
    "        z_disc = Input(shape=(self.latent_dim,))\n",
    "        # Generate image based of noise (fake sample)\n",
    "        fake_img = self.generator(z_disc)\n",
    "\n",
    "        # Discriminator determines validity of the real and fake images\n",
    "        fake = self.discriminator(fake_img)\n",
    "        valid = self.discriminator(real_img)\n",
    "\n",
    "        # Construct weighted average between real and fake images\n",
    "        interpolated_img = RandomWeightedAverage()([real_img, fake_img])\n",
    "        # Determine validity of weighted sample\n",
    "        validity_interpolated = self.discriminator(interpolated_img)\n",
    "\n",
    "        # Use Python partial to provide loss function with additional\n",
    "        # 'averaged_samples' argument\n",
    "        partial_gp_loss = partial(self.gradient_penalty_loss,\n",
    "                          averaged_samples=interpolated_img)\n",
    "        partial_gp_loss.__name__ = 'gradient_penalty' # Keras requires function names\n",
    "\n",
    "        self.discriminator_model = Model(inputs=[real_img, z_disc],\n",
    "                            outputs=[valid, fake, validity_interpolated])\n",
    "        self.discriminator_model.compile(loss=[self.wasserstein_loss,\n",
    "                                              self.wasserstein_loss,\n",
    "                                              partial_gp_loss],\n",
    "                                        optimizer=optimizer,\n",
    "                                        loss_weights=[1, 1, 10])\n",
    "        #-------------------------------\n",
    "        # Construct Computational Graph\n",
    "        #         for Generator\n",
    "        #-------------------------------\n",
    "\n",
    "        # For the generator we freeze the discriminator's layers\n",
    "        self.discriminator.trainable = False\n",
    "        self.generator.trainable = True\n",
    "\n",
    "        # Sampled noise for input to generator\n",
    "        z_gen = Input(shape=(self.latent_dim,))\n",
    "        # Generate images based of noise\n",
    "        img = self.generator(z_gen)\n",
    "        # Discriminator determines validity\n",
    "        valid = self.discriminator(img)\n",
    "        # Defines generator model\n",
    "        self.generator_model = Model(z_gen, valid)\n",
    "        self.generator_model.compile(loss=self.wasserstein_loss, optimizer=optimizer)\n",
    "\n",
    "\n",
    "    def gradient_penalty_loss(self, y_true, y_pred, averaged_samples):\n",
    "        \"\"\"\n",
    "        Computes gradient penalty based on prediction and weighted real / fake samples\n",
    "        \"\"\"\n",
    "        gradients = K.gradients(y_pred, averaged_samples)[0]\n",
    "        # compute the euclidean norm by squaring ...\n",
    "        gradients_sqr = K.square(gradients)\n",
    "        #   ... summing over the rows ...\n",
    "        gradients_sqr_sum = K.sum(gradients_sqr,\n",
    "                                  axis=np.arange(1, len(gradients_sqr.shape)))\n",
    "        #   ... and sqrt\n",
    "        gradient_l2_norm = K.sqrt(gradients_sqr_sum)\n",
    "        # compute lambda * (1 - ||grad||)^2 still for each single sample\n",
    "        gradient_penalty = K.square(1 - gradient_l2_norm)\n",
    "        # return the mean as loss over all the batch samples\n",
    "        return K.mean(gradient_penalty)\n",
    "\n",
    "\n",
    "    def wasserstein_loss(self, y_true, y_pred):\n",
    "        return K.mean(y_true * y_pred)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Dense(4*4, input_shape=(128,)))\n",
    "        model.add(Reshape((4,4,1)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(ReLU())\n",
    "        \n",
    "        model.add(Conv2DTranspose(64, kernel_size=(5,5), strides=(2,2), padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(ReLU())\n",
    "\n",
    "        model.add(Conv2DTranspose(32, kernel_size=(5,5), strides=(2,2), padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(ReLU())\n",
    "\n",
    "        model.add(Conv2DTranspose(16, kernel_size=(5,5), strides=(2,2), padding='same'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(ReLU())\n",
    "\n",
    "        model.add(Conv2DTranspose(1, kernel_size=(5,5), strides=(2,2), padding='same'))\n",
    "        model.add(Activation('tanh'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=(self.latent_dim,))\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(16, kernel_size=(3,3), strides=(2,2), input_shape=(64, 64, 1)))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=(3,3), strides=(2,2)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Conv2D(64, kernel_size=(3,3), strides=(2,2)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Conv2D(128, kernel_size=(3,3), strides=(2,2)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=self.img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size, sample_interval=50):\n",
    "        X_train = np.random.normal(0, 0.1, [batch_size*256, 64, 64, 1])\n",
    "\n",
    "        # Adversarial ground truths\n",
    "        valid = -np.ones((batch_size, 1))\n",
    "        fake =  np.ones((batch_size, 1))\n",
    "        dummy = np.zeros((batch_size, 1)) # Dummy gt for gradient penalty\n",
    "        for epoch in range(epochs):\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "            imgs = X_train[idx]\n",
    "            \n",
    "            # Sample generator input\n",
    "            noise = np.random.normal(0, 1, (batch_size, self.latent_dim))\n",
    "            \n",
    "            # Train the discriminator\n",
    "            d_loss = self.discriminator_model.train_on_batch([imgs, noise],\n",
    "                                                                [valid, fake, dummy])\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "            g_loss = self.generator_model.train_on_batch(noise, valid)\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f] [G loss: %f]\" % (epoch, d_loss[0], g_loss))\n",
    "\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % sample_interval == 0:\n",
    "                self.sample_images(epoch)\n",
    "\n",
    "    def sample_images(self, epoch):\n",
    "        r, c = 5, 5\n",
    "        noise = np.random.normal(0, 1, (r * c, self.latent_dim))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                print(np.mean(gen_imgs[cnt, :,:,0]))\n",
    "                print(np.std(gen_imgs[cnt, :,:,0]))\n",
    "                axs[i,j].imshow(gen_imgs[cnt, :,:,0], cmap='gray')\n",
    "                axs[i,j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(\"images/image_%d.png\" % epoch)\n",
    "        plt.close()\n",
    "\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    wgan = GCBD()\n",
    "    wgan.train(epochs=2000, batch_size=32, sample_interval=400)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
